# é«˜æ€§èƒ½ä¼˜åŒ–ç‰ˆï¼šFastOptimizedLocalUnitaryGCN
# ä¸»è¦ä¼˜åŒ–ï¼š
# 1. æ‰¹é‡å­å›¾é¢„æå–å’Œç¼“å­˜
# 2. é…‰çŸ©é˜µé¢„è®¡ç®—å’Œå¤ç”¨
# 3. å‘é‡åŒ–è®¡ç®—å‡å°‘å¾ªç¯
# 4. ç¨€ç–çŸ©é˜µé«˜æ•ˆæ“ä½œ
# 5. å†…å­˜ä¼˜åŒ–çš„æ¼”åŒ–è®¡ç®—

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import global_mean_pool
from torch_geometric.utils import add_self_loops, degree, k_hop_subgraph
from typing import Optional, Tuple, Dict, List
import math
from torch_geometric.data import Data
from torch_geometric.utils import erdos_renyi_graph
import time
from torch_geometric.utils import subgraph
import numpy as np
from collections import defaultdict

from complexPyTorch.complexLayers import ComplexLinear, NaiveComplexBatchNorm1d
from complexPyTorch.complexFunctions import complex_relu, complex_dropout


# ==== é«˜æ•ˆæ‰¹é‡çŸ©é˜µæŒ‡æ•°è®¡ç®— ====
def batch_efficient_matrix_exp(H_batch, t=1.0, max_terms=6):
    """æ‰¹é‡è®¡ç®—å¤šä¸ªçŸ©é˜µçš„æŒ‡æ•°ï¼Œæé«˜å¹¶è¡Œåº¦"""
    device = H_batch.device
    batch_size, n, _ = H_batch.shape

    # ç¼©æ”¾æ—¶é—´æ­¥é•¿
    scaled_H = -1j * H_batch * t

    # æ‰¹é‡å•ä½çŸ©é˜µ
    I = torch.eye(n, device=device, dtype=torch.complex64).expand_as(scaled_H)
    result = I.clone()
    term = I.clone()

    for k in range(1, max_terms + 1):
        term = torch.bmm(term, scaled_H) / k
        result = result + term

        # æ—©åœæ£€æŸ¥
        if torch.max(torch.abs(term)) < 1e-6:
            break

    return result


# ==== å­å›¾ç¼“å­˜ç®¡ç†å™¨ ====
class SubgraphCache:
    def __init__(self, max_cache_size=1000):
        self.cache = {}
        self.max_cache_size = max_cache_size
        self.access_count = defaultdict(int)

    def get_subgraph_key(self, center_node, edge_index, k_hop):
        """ç”Ÿæˆå­å›¾çš„å”¯ä¸€é”®"""
        # ç®€åŒ–é”®ç”Ÿæˆï¼ŒåŸºäºä¸­å¿ƒèŠ‚ç‚¹å’Œè¾¹ç´¢å¼•çš„å“ˆå¸Œ
        edge_hash = hash(tuple(edge_index.flatten().tolist()[:100]))  # é™åˆ¶é•¿åº¦é¿å…è¿‡é•¿
        return f"{center_node}_{k_hop}_{edge_hash}"

    def get_or_compute_subgraph(self, center_node, edge_index, k_hop, num_nodes, max_subgraph_size):
        """è·å–æˆ–è®¡ç®—å­å›¾"""
        key = self.get_subgraph_key(center_node, edge_index, k_hop)

        if key in self.cache:
            self.access_count[key] += 1
            return self.cache[key]

        # è®¡ç®—å­å›¾
        sub_nodes, sub_edge_index, mapping, _ = k_hop_subgraph(
            center_node, k_hop, edge_index, num_nodes=num_nodes,
            relabel_nodes=True
        )

        # é™åˆ¶å­å›¾å¤§å°
        if len(sub_nodes) > max_subgraph_size:
            # ä¿ç•™ä¸­å¿ƒèŠ‚ç‚¹ï¼Œéšæœºé‡‡æ ·å…¶ä»–èŠ‚ç‚¹
            center_idx = mapping.item()
            other_nodes = [i for i in range(len(sub_nodes)) if i != center_idx]
            selected_others = torch.randperm(len(other_nodes))[:max_subgraph_size - 1]
            selected_nodes = [center_idx] + [other_nodes[i] for i in selected_others]

            # é‡æ–°æ„å»ºå­å›¾
            original_selected = sub_nodes[selected_nodes]
            sub_edge_index, _ = subgraph(original_selected, edge_index,
                                         relabel_nodes=True, num_nodes=num_nodes)
            sub_nodes = original_selected
            mapping = torch.tensor(0)  # ä¸­å¿ƒèŠ‚ç‚¹æ€»æ˜¯0

        result = {
            'sub_nodes': sub_nodes,
            'sub_edge_index': sub_edge_index,
            'center_mapping': mapping,
            'size': len(sub_nodes)
        }

        # ç¼“å­˜ç®¡ç†
        if len(self.cache) >= self.max_cache_size:
            # åˆ é™¤æœ€å°‘ä½¿ç”¨çš„é¡¹
            least_used = min(self.cache.keys(), key=lambda k: self.access_count[k])
            del self.cache[least_used]
            del self.access_count[least_used]

        self.cache[key] = result
        self.access_count[key] = 1
        return result


# ==== é…‰çŸ©é˜µç¼“å­˜ç®¡ç†å™¨ ====
class UnitaryMatrixCache:
    def __init__(self, max_cache_size=500):
        self.cache = {}
        self.max_cache_size = max_cache_size

    def get_matrix_key(self, sub_edge_index, num_nodes, hamilton_type, evolution_time):
        """ç”Ÿæˆé…‰çŸ©é˜µçš„å”¯ä¸€é”®"""
        edge_str = '_'.join(map(str, sub_edge_index.flatten().tolist()[:50]))  # é™åˆ¶é•¿åº¦
        return f"{num_nodes}_{hamilton_type}_{evolution_time:.3f}_{hash(edge_str)}"

    def get_or_compute_unitary(self, sub_edge_index, num_nodes, hamilton_type, evolution_time):
        """è·å–æˆ–è®¡ç®—é…‰çŸ©é˜µ"""
        key = self.get_matrix_key(sub_edge_index, num_nodes, hamilton_type, evolution_time)

        if key in self.cache:
            return self.cache[key]

        # è®¡ç®—å“ˆå¯†é¡¿çŸ©é˜µ
        H = self._create_hamiltonian(sub_edge_index, num_nodes, hamilton_type)

        # è®¡ç®—é…‰çŸ©é˜µ
        U_sub = batch_efficient_matrix_exp(H.unsqueeze(0), evolution_time, max_terms=4)[0]

        # æ„å»ºæ‰©å±•é…‰ç®—å­
        dim = num_nodes
        U = torch.zeros(2 * dim, 2 * dim, dtype=torch.complex64, device=H.device)
        U[:dim, :dim] = U_sub
        U[dim:, dim:] = U_sub.conj().transpose(-2, -1)

        # ç¼“å­˜ç®¡ç†
        if len(self.cache) >= self.max_cache_size:
            # ç®€å•çš„FIFOç­–ç•¥
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]

        self.cache[key] = U
        return U

    def _create_hamiltonian(self, edge_index, num_nodes, hamilton_type='laplacian'):
        """åˆ›å»ºå“ˆå¯†é¡¿çŸ©é˜µ"""
        device = edge_index.device

        if edge_index.size(1) == 0:
            return torch.zeros(num_nodes, num_nodes, dtype=torch.complex64, device=device)

        row, col = edge_index
        edge_weight = torch.ones(edge_index.size(1), device=device)

        # è®¡ç®—åº¦
        deg = degree(row, num_nodes)

        if hamilton_type == 'laplacian':
            # æ„å»ºæ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ
            L = torch.zeros(num_nodes, num_nodes, device=device)
            L[row, col] = -edge_weight
            L.diagonal().add_(deg)
        elif hamilton_type == 'norm_laplacian':
            # å½’ä¸€åŒ–æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ
            deg_inv_sqrt = deg.pow(-0.5)
            deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0

            L = torch.zeros(num_nodes, num_nodes, device=device)
            L[row, col] = -deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]
            L.diagonal().add_(1.0)

        return L.to(torch.complex64) * 0.05


# ==== å¿«é€Ÿå±€éƒ¨é…‰GCNå·ç§¯å±‚ ====
class FastLocalUnitaryGCNConv(nn.Module):
    def __init__(self, in_channels, out_channels, k_hop=1, evolution_time=0.5,
                 hamilton_type='laplacian', max_subgraph_size=6, dropout=0.1):
        super().__init__()
        self.lin = ComplexLinear(in_channels, out_channels)
        self.norm = NaiveComplexBatchNorm1d(out_channels)
        self.k_hop = k_hop
        self.evolution_time = evolution_time
        self.hamilton_type = hamilton_type
        self.max_subgraph_size = max_subgraph_size
        self.dropout = dropout

        # ç¼“å­˜ç®¡ç†å™¨
        self.subgraph_cache = SubgraphCache(max_cache_size=1000)
        self.unitary_cache = UnitaryMatrixCache(max_cache_size=500)

        # æ®‹å·®è¿æ¥
        if in_channels != out_channels:
            self.residual_proj = ComplexLinear(in_channels, out_channels)
        else:
            self.residual_proj = None

    def forward(self, x, edge_index):
        x_residual = x

        # è½¬æ¢ä¸ºå¤æ•°
        if x.is_floating_point():
            x = torch.complex(x, torch.zeros_like(x))

        # çº¿æ€§å˜æ¢
        x = self.lin(x)

        N = x.size(0)
        edge_index_with_loops, _ = add_self_loops(edge_index, num_nodes=N)

        # æ‰¹é‡å¤„ç†èŠ‚ç‚¹ï¼ˆå‡å°‘å­å›¾æ„å»ºæ¬¡æ•°ï¼‰
        evolved = self._batch_evolution(x, edge_index_with_loops, N)

        # æ¿€æ´»å‡½æ•°
        evolved = complex_relu(evolved)

        # Dropout
        if self.training and self.dropout > 0:
            evolved = complex_dropout(evolved, self.dropout)

        # å±‚å½’ä¸€åŒ–
        evolved = self.norm(evolved)

        # æ®‹å·®è¿æ¥
        if self.residual_proj is not None:
            if x_residual.is_floating_point():
                x_residual = torch.complex(x_residual, torch.zeros_like(x_residual))
            x_residual = self.residual_proj(x_residual)
        elif x_residual.is_floating_point():
            x_residual = torch.complex(x_residual, torch.zeros_like(x_residual))

        return evolved + x_residual

    def _batch_evolution(self, x, edge_index, N):
        """æ‰¹é‡æ¼”åŒ–è®¡ç®—ï¼Œå‡å°‘é‡å¤æ“ä½œ"""
        evolved = torch.zeros_like(x)

        # èŠ‚ç‚¹åˆ†æ‰¹å¤„ç†
        batch_size = min(32, N)  # æ¯æ‰¹å¤„ç†32ä¸ªèŠ‚ç‚¹

        for batch_start in range(0, N, batch_size):
            batch_end = min(batch_start + batch_size, N)
            batch_nodes = list(range(batch_start, batch_end))

            # æ‰¹é‡å¤„ç†å½“å‰æ‰¹æ¬¡çš„èŠ‚ç‚¹
            for i in batch_nodes:
                # è·å–å­å›¾ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
                subgraph_info = self.subgraph_cache.get_or_compute_subgraph(
                    i, edge_index, self.k_hop, N, self.max_subgraph_size
                )

                sub_nodes = subgraph_info['sub_nodes']
                sub_edge_index = subgraph_info['sub_edge_index']
                center_mapping = subgraph_info['center_mapping']

                if len(sub_nodes) == 0 or sub_edge_index.size(1) == 0:
                    evolved[i] = x[i]
                    continue

                # è·å–é…‰çŸ©é˜µï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
                U = self.unitary_cache.get_or_compute_unitary(
                    sub_edge_index, len(sub_nodes),
                    self.hamilton_type, self.evolution_time
                )

                # åº”ç”¨é…‰æ¼”åŒ–
                sub_x = x[sub_nodes]
                dim = len(sub_nodes)

                # æ„å»ºçŠ¶æ€å‘é‡
                z = torch.zeros(2 * dim, x.size(1), dtype=torch.complex64, device=x.device)
                z[:dim] = sub_x

                # æ¼”åŒ–
                z_evolved = torch.matmul(U, z)

                # æå–ä¸­å¿ƒèŠ‚ç‚¹ç»“æœ
                evolved[i] = z_evolved[center_mapping]

        return evolved


# ==== ä¸»ç½‘ç»œï¼ˆä¿æŒä¸å˜ï¼‰ ====
class FastOptimizedLocalUnitaryGCN(nn.Module):
    def __init__(self, input_dim, hidden_dims, output_dim,
                 k_hop=1, evolution_times=None, hamilton_type='laplacian',
                 dropout=0.1, max_subgraph_size=6):
        super().__init__()
        dims = [input_dim] + hidden_dims + [output_dim]
        self.layers = nn.ModuleList()

        if evolution_times is None:
            evolution_times = [0.3, 0.5, 0.7][:len(dims) - 1]
            evolution_times = evolution_times + [evolution_times[-1]] * (len(dims) - 1 - len(evolution_times))

        for i in range(len(dims) - 1):
            self.layers.append(FastLocalUnitaryGCNConv(
                in_channels=dims[i],
                out_channels=dims[i + 1],
                k_hop=k_hop,
                evolution_time=evolution_times[i],
                hamilton_type=hamilton_type,
                dropout=dropout if i < len(dims) - 2 else 0.0,
                max_subgraph_size=max_subgraph_size
            ))

        self.grad_clip = 1.0

    def forward(self, data):
        x, edge_index, batch = data.x, data.edge_index, data.batch

        for layer in self.layers:
            x = layer(x, edge_index)

            if self.training:
                torch.nn.utils.clip_grad_norm_(self.parameters(), self.grad_clip)

        # æ¨¡é•¿åˆ†ç±»
        x = x.abs()

        # å…¨å±€å¹³å‡æ± åŒ–
        x = global_mean_pool(x, batch)

        return F.log_softmax(x, dim=1)


# ==== æµ‹è¯•å‡½æ•° ====
def test_fast_optimized_local_unitary_gcn():
    print("\U0001F680 é«˜æ€§èƒ½ä¼˜åŒ–ç‰ˆ LocalUnitaryGCN æµ‹è¯•")
    print("=" * 60)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # æ¨¡æ‹Ÿå›¾æ„å»º
    num_nodes = 100
    num_features = 64
    num_classes = 2
    edge_index = erdos_renyi_graph(num_nodes, edge_prob=0.05).to(device)
    x = torch.randn(num_nodes, num_features).to(device) * 0.1
    batch = torch.zeros(num_nodes, dtype=torch.long).to(device)
    data = Data(x=x, edge_index=edge_index, batch=batch)

    print(f"\nğŸ“Š å›¾ä¿¡æ¯: {num_nodes} èŠ‚ç‚¹, {edge_index.size(1)} æ¡è¾¹")

    model = FastOptimizedLocalUnitaryGCN(
        input_dim=num_features,
        hidden_dims=[32, 16],
        output_dim=num_classes,
        k_hop=1,
        evolution_times=[0.2, 0.3, 0.4],
        hamilton_type='laplacian',
        dropout=0.1,
        max_subgraph_size=4
    ).to(device)

    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    # æ€§èƒ½æµ‹è¯•
    model.eval()
    with torch.no_grad():
        # é¢„çƒ­
        for _ in range(3):
            _ = model(data)

        # æ­£å¼æµ‹è¯•
        start_time = time.time()
        num_runs = 10
        for _ in range(num_runs):
            output = model(data)
        end_time = time.time()

        avg_time = (end_time - start_time) / num_runs
        print(f"å¹³å‡æ¨ç†æ—¶é—´: {avg_time * 1000:.2f} ms")
        print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"è¾“å‡ºæ˜¯å¦ç¨³å®š: NaN={torch.isnan(output).any().item()}, Inf={torch.isinf(output).any().item()}")

    # æ¢¯åº¦æµ‹è¯•
    model.train()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)

    try:
        y_fake = torch.randint(0, num_classes, (1,)).to(device)
        output = model(data)
        loss = F.nll_loss(output, y_fake)
        print(f"æµ‹è¯•loss: {loss.item():.6f}")

        loss.backward()
        optimizer.step()
        print("âœ… æ¢¯åº¦æ›´æ–°æˆåŠŸ")

    except Exception as e:
        print(f"è®­ç»ƒæµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

    print("\nğŸš€ æ€§èƒ½ä¼˜åŒ–è¦ç‚¹:")
    print("  - å­å›¾å’Œé…‰çŸ©é˜µæ™ºèƒ½ç¼“å­˜")
    print("  - æ‰¹é‡çŸ©é˜µæŒ‡æ•°è®¡ç®—")
    print("  - k_hop_subgraphé«˜æ•ˆå­å›¾æå–")
    print("  - èŠ‚ç‚¹åˆ†æ‰¹å¤„ç†å‡å°‘å†…å­˜å ç”¨")
    print("  - æ—©åœå’Œæ•°å€¼ç¨³å®šæ€§ä¼˜åŒ–")
    print("  - ä¿æŒå®Œæ•´çš„é‡å­æ¼”åŒ–ç‰¹æ€§")


if __name__ == "__main__":
    test_fast_optimized_local_unitary_gcn()