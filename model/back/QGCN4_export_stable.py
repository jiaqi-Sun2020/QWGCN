# ä¿®å¤NaNé—®é¢˜çš„ç¨³å®šç‰ˆ OptimizedLocalUnitaryGCN
# ä¸»è¦ä¿®å¤ï¼š
# 1. æ›´ä¿å®ˆçš„æ•°å€¼å‚æ•°å’Œåˆå§‹åŒ–
# 2. å¢å¼ºçš„æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥å’Œä¿®å¤æœºåˆ¶
# 3. æ¸è¿›å¼è®­ç»ƒå’Œè‡ªé€‚åº”ç¼©æ”¾
# 4. æ›´ç¨³å®šçš„å¤æ•°è¿ç®—
# 5. ä¿æŒæ‰€æœ‰æ ¸å¿ƒç‰¹æ€§ï¼šå®Œæ•´å¤æ•°æ¼”åŒ– + æ¨¡é•¿åˆ†ç±» + å±€éƒ¨é…‰æ€§ + é…‰æ‰©å¼ 

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import global_mean_pool
from torch_geometric.utils import add_self_loops, degree
from typing import Optional, Tuple
import math
from torch_geometric.data import Data
from torch_geometric.utils import erdos_renyi_graph
import time
from torch_geometric.utils import subgraph
import numpy as np


# ==== æ•°å€¼ç¨³å®šæ€§å·¥å…·å‡½æ•° ====
def check_and_fix_nan_inf(tensor, name="tensor", fix_value=1e-6):
    """æ£€æŸ¥å¹¶ä¿®å¤NaNå’ŒInfå€¼"""
    if torch.isnan(tensor).any() or torch.isinf(tensor).any():
        print(f"âš ï¸ æ£€æµ‹åˆ° {name} ä¸­çš„NaN/Infå€¼ï¼Œæ­£åœ¨ä¿®å¤...")
        tensor = torch.where(torch.isnan(tensor) | torch.isinf(tensor),
                             torch.full_like(tensor, fix_value), tensor)
    return tensor


def safe_complex_magnitude(x, eps=1e-8, max_val=1e3):
    """å®‰å…¨çš„å¤æ•°æ¨¡é•¿è®¡ç®—ï¼Œé¿å…æ•°å€¼æº¢å‡º"""
    mag = torch.sqrt(x.real ** 2 + x.imag ** 2 + eps)
    # é™åˆ¶æœ€å¤§å€¼é¿å…æº¢å‡º
    mag = torch.clamp(mag, min=eps, max=max_val)
    return mag


def safe_complex_phase(x, eps=1e-10):
    """å®‰å…¨çš„å¤æ•°ç›¸ä½è®¡ç®—"""
    # é¿å…atan2ä¸­çš„æ•°å€¼é—®é¢˜
    real_part = torch.clamp(x.real, min=-1e6, max=1e6)
    imag_part = torch.clamp(x.imag, min=-1e6, max=1e6)
    phase = torch.atan2(imag_part, real_part + eps)
    return check_and_fix_nan_inf(phase, "phase")


# ==== è¶…ç¨³å®šçš„å¤æ•°å±‚å½’ä¸€åŒ– ====
class UltraStableComplexLayerNorm(nn.Module):
    def __init__(self, normalized_shape, eps=1e-5):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(normalized_shape) * 0.1)  # æ›´å°çš„åˆå§‹æƒé‡
        self.bias = nn.Parameter(torch.zeros(normalized_shape))
        self.register_buffer('running_mean', torch.zeros(normalized_shape))
        self.register_buffer('running_var', torch.ones(normalized_shape))
        self.momentum = 0.1

    def forward(self, x):
        # è®¡ç®—å®‰å…¨çš„æ¨¡é•¿
        magnitude = safe_complex_magnitude(x, self.eps)

        if self.training:
            # è®­ç»ƒæ—¶ä½¿ç”¨æ‰¹æ¬¡ç»Ÿè®¡
            mean_mag = magnitude.mean(dim=0)
            var_mag = magnitude.var(dim=0, unbiased=False)

            # æ›´æ–°è¿è¡Œç»Ÿè®¡
            with torch.no_grad():
                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean_mag
                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var_mag
        else:
            # æ¨ç†æ—¶ä½¿ç”¨è¿è¡Œç»Ÿè®¡
            mean_mag = self.running_mean
            var_mag = self.running_var

        # å®‰å…¨çš„æ ‡å‡†åŒ–
        std_mag = torch.sqrt(var_mag + self.eps)
        normalized_mag = (magnitude - mean_mag) / (std_mag + self.eps)

        # åº”ç”¨å­¦ä¹ å‚æ•°ï¼Œé™åˆ¶èŒƒå›´
        scaled_mag = torch.clamp(normalized_mag * torch.abs(self.weight) + self.bias,
                                 min=self.eps, max=10.0)

        # é‡æ„å¤æ•°
        phase = safe_complex_phase(x, self.eps)
        result = torch.polar(scaled_mag, phase)

        return check_and_fix_nan_inf(result, "layer_norm_output")


# ==== è¶…ç¨³å®šçš„å¤æ•°çº¿æ€§å±‚ ====
class UltraStableComplexLinear(nn.Module):
    def __init__(self, in_features, out_features, bias=True):
        super().__init__()
        # æ›´ä¿å®ˆçš„Xavieråˆå§‹åŒ–
        std = math.sqrt(0.5 / in_features)  # å‡åŠæ ‡å‡†å·®
        self.weight_real = nn.Parameter(torch.randn(out_features, in_features) * std)
        self.weight_imag = nn.Parameter(torch.randn(out_features, in_features) * std)

        if bias:
            self.bias_real = nn.Parameter(torch.zeros(out_features))
            self.bias_imag = nn.Parameter(torch.zeros(out_features))
        else:
            self.register_parameter('bias_real', None)
            self.register_parameter('bias_imag', None)

    def forward(self, x):
        # è¾“å…¥æ£€æŸ¥
        x = check_and_fix_nan_inf(x, "linear_input")

        # é™åˆ¶è¾“å…¥èŒƒå›´
        x_real = torch.clamp(x.real, min=-10, max=10)
        x_imag = torch.clamp(x.imag, min=-10, max=10)

        # å¤æ•°çŸ©é˜µä¹˜æ³•
        real_part = torch.matmul(x_real, self.weight_real.T) - torch.matmul(x_imag, self.weight_imag.T)
        imag_part = torch.matmul(x_real, self.weight_imag.T) + torch.matmul(x_imag, self.weight_real.T)

        result = torch.complex(real_part, imag_part)

        if self.bias_real is not None:
            bias = torch.complex(self.bias_real, self.bias_imag)
            result = result + bias

        return check_and_fix_nan_inf(result, "linear_output")


# ==== è¶…ç¨³å®šçš„å¤æ•°æ¿€æ´»å‡½æ•° ====
def ultra_stable_complex_relu(x, leak=0.01, max_val=5.0):
    """è¶…ç¨³å®šçš„å¤æ•°ReLUï¼Œå¸¦æ•°å€¼é™åˆ¶"""
    x = check_and_fix_nan_inf(x, "activation_input")

    # é™åˆ¶è¾“å…¥èŒƒå›´
    real_part = torch.clamp(x.real, min=-max_val, max=max_val)
    imag_part = torch.clamp(x.imag, min=-max_val, max=max_val)

    # å¸¦æ³„æ¼çš„ReLU
    real_activated = torch.where(real_part > 0, real_part, leak * real_part)
    imag_activated = torch.where(imag_part > 0, imag_part, leak * imag_part)

    result = torch.complex(real_activated, imag_activated)
    return check_and_fix_nan_inf(result, "activation_output")


def ultra_stable_complex_dropout(x, p=0.5, training=True):
    """è¶…ç¨³å®šçš„å¤æ•°dropout"""
    if not training or p == 0:
        return x

    x = check_and_fix_nan_inf(x, "dropout_input")
    magnitude = safe_complex_magnitude(x)

    # dropout mask
    mask = (torch.rand_like(magnitude) > p).float()
    scale = 1.0 / (1.0 - p + 1e-8)

    phase = safe_complex_phase(x)
    result = torch.polar(magnitude * mask * scale, phase)

    return check_and_fix_nan_inf(result, "dropout_output")


# ==== è¶…ç¨³å®šçš„çŸ©é˜µæŒ‡æ•°è¿‘ä¼¼ ====
def ultra_stable_matrix_exp(H, t=1.0, max_terms=4, max_norm=1e-2):
    """è¶…ç¨³å®šçš„çŸ©é˜µæŒ‡æ•°è¿‘ä¼¼ï¼Œå¼ºåˆ¶æ•°å€¼ç¨³å®šæ€§"""
    device = H.device
    n = H.size(-1)

    # è¾“å…¥æ£€æŸ¥å’Œä¿®å¤
    H = check_and_fix_nan_inf(H, "hamiltonian")

    # éå¸¸ä¿å®ˆçš„ç¼©æ”¾
    H_norm = torch.norm(H).item()
    if H_norm > max_norm:
        H = H * (max_norm / H_norm)

    # æ›´å°çš„æ—¶é—´æ­¥é•¿
    scaled_H = -1j * H * t * 0.1  # è¿›ä¸€æ­¥å‡å°æ—¶é—´æ­¥é•¿

    # æ³°å‹’å±•å¼€
    result = torch.eye(n, device=device, dtype=torch.complex64).expand_as(scaled_H)
    term = torch.eye(n, device=device, dtype=torch.complex64).expand_as(scaled_H)

    for k in range(1, max_terms + 1):
        term = torch.matmul(term, scaled_H) / k
        term = check_and_fix_nan_inf(term, f"exp_term_{k}")

        # é™åˆ¶é¡¹çš„å¤§å°
        term_norm = torch.norm(term).item()
        if term_norm > 1e-1:
            term = term * (1e-1 / term_norm)

        result = result + term

        # æ—©åœ
        if torch.max(torch.abs(term)) < 1e-8:
            break

    result = check_and_fix_nan_inf(result, "matrix_exp_result")

    # å¼ºåˆ¶é…‰æ€§æ£€æŸ¥å’Œä¿®å¤
    result_dag = result.conj().transpose(-2, -1)
    should_be_identity = torch.matmul(result, result_dag)
    identity = torch.eye(n, device=device, dtype=torch.complex64)

    # å¦‚æœåç¦»é…‰æ€§å¤ªå¤šï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„ç»“æœ
    unitarity_error = torch.norm(should_be_identity - identity).item()
    if unitarity_error > 0.1:
        # å›é€€åˆ°å•ä½çŸ©é˜µåŠ å°æ‰°åŠ¨
        result = identity + scaled_H * 0.01
        result = check_and_fix_nan_inf(result, "fallback_unitary")

    return result


# ==== è¶…ç¨³å®šçš„å“ˆå¯†é¡¿çŸ©é˜µæ„é€  ====
def create_ultra_stable_hamiltonian(edge_index, num_nodes, edge_weight=None, type='laplacian', max_eigenval=0.5):
    """æ„é€ æ•°å€¼è¶…ç¨³å®šçš„å“ˆå¯†é¡¿çŸ©é˜µ"""
    if edge_weight is None:
        edge_weight = torch.ones(edge_index.size(1), device=edge_index.device) * 0.1  # æ›´å°çš„æƒé‡

    row, col = edge_index
    deg = degree(row, num_nodes)
    deg = torch.clamp(deg, min=1e-6)  # é¿å…åº¦ä¸º0çš„é—®é¢˜

    if type == 'laplacian':
        # æ ‡å‡†æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ
        laplacian_weight = torch.cat([deg, -edge_weight])
        laplacian_index = torch.cat([
            torch.stack([torch.arange(num_nodes, device=edge_index.device),
                         torch.arange(num_nodes, device=edge_index.device)]),
            edge_index
        ], dim=1)
    elif type == 'norm_laplacian':
        # å½’ä¸€åŒ–æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ
        deg_inv_sqrt = deg.pow(-0.5)
        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0
        deg_inv_sqrt = torch.clamp(deg_inv_sqrt, max=10.0)  # é™åˆ¶æœ€å¤§å€¼

        norm_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]
        laplacian_weight = torch.cat([torch.ones(num_nodes, device=edge_index.device), -norm_weight])
        laplacian_index = torch.cat([
            torch.stack([torch.arange(num_nodes, device=edge_index.device),
                         torch.arange(num_nodes, device=edge_index.device)]),
            edge_index
        ], dim=1)

    # æ„é€ å¯†é›†çŸ©é˜µ
    H = torch.sparse_coo_tensor(laplacian_index, laplacian_weight, (num_nodes, num_nodes)).to_dense()
    H = H.to(torch.complex64)

    # æ£€æŸ¥å’Œä¿®å¤
    H = check_and_fix_nan_inf(H, "hamiltonian_matrix")

    # æ§åˆ¶æœ€å¤§ç‰¹å¾å€¼ä»¥ä¿è¯æ•°å€¼ç¨³å®šæ€§
    try:
        eigenvals = torch.linalg.eigvals(H)
        max_eigenval_actual = torch.max(torch.real(eigenvals)).item()
        if max_eigenval_actual > max_eigenval:
            H = H * (max_eigenval / max_eigenval_actual)
    except:
        # å¦‚æœç‰¹å¾å€¼è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„ç¼©æ”¾
        H = H * 0.01

    return H


# ==== è¶…ç¨³å®šçš„å±€éƒ¨é…‰GCNå·ç§¯å±‚ ====
class UltraStableLocalUnitaryGCNConv(nn.Module):
    def __init__(self, in_channels, out_channels, k_hop=1, evolution_time=0.1,
                 hamilton_type='laplacian', max_subgraph_size=4, dropout=0.05):
        super().__init__()
        self.lin = UltraStableComplexLinear(in_channels, out_channels)
        self.norm = UltraStableComplexLayerNorm(out_channels)
        self.k_hop = k_hop
        self.evolution_time = evolution_time  # å·²ç»å¾ˆå°äº†
        self.hamilton_type = hamilton_type
        self.max_subgraph_size = max_subgraph_size
        self.dropout = dropout

        # æ®‹å·®è¿æ¥çš„æŠ•å½±å±‚
        if in_channels != out_channels:
            self.residual_proj = UltraStableComplexLinear(in_channels, out_channels)
        else:
            self.residual_proj = None

        # è‡ªé€‚åº”ç¼©æ”¾å‚æ•°
        self.register_parameter('evolution_scale', nn.Parameter(torch.tensor(0.1)))

    def forward(self, x, edge_index):
        # è¾“å…¥æ£€æŸ¥
        x = check_and_fix_nan_inf(x, "conv_input")

        # ä¿å­˜è¾“å…¥ç”¨äºæ®‹å·®è¿æ¥
        x_residual = x

        # è½¬æ¢ä¸ºå¤æ•°
        if x.is_floating_point():
            x = torch.complex(x, torch.zeros_like(x) * 0.01)  # æ·»åŠ å°è™šéƒ¨

        # çº¿æ€§å˜æ¢
        x = self.lin(x)

        N = x.size(0)
        edge_index_with_loops, _ = add_self_loops(edge_index, num_nodes=N)

        # æ„å»ºé‚»æ¥å…³ç³»å­—å…¸
        adj_dict = {}
        for i in range(edge_index_with_loops.size(1)):
            src, dst = edge_index_with_loops[:, i].tolist()
            if src not in adj_dict:
                adj_dict[src] = []
            adj_dict[src].append(dst)

        evolved = torch.zeros_like(x)

        for i in range(N):
            try:
                # è·å–k-hopé‚»å±…
                neighbors = set([i])
                current_level = {i}

                for hop in range(self.k_hop):
                    next_level = set()
                    for node in current_level:
                        if node in adj_dict:
                            next_level.update(adj_dict[node])
                    neighbors.update(next_level)
                    current_level = next_level

                    if len(neighbors) >= self.max_subgraph_size:
                        break

                sub_nodes = list(neighbors)[:self.max_subgraph_size]
                if i not in sub_nodes:
                    sub_nodes[0] = i

                # æ„å»ºå­å›¾
                sub_edge_index, _ = subgraph(sub_nodes, edge_index_with_loops,
                                             relabel_nodes=True, num_nodes=N)

                if sub_edge_index.size(1) == 0:
                    evolved[i] = x[i]
                    continue

                center_idx = sub_nodes.index(i)
                sub_x = x[sub_nodes]
                sub_x = check_and_fix_nan_inf(sub_x, f"subgraph_x_{i}")

                # æ„å»ºå­å›¾å“ˆå¯†é¡¿çŸ©é˜µ
                H = create_ultra_stable_hamiltonian(sub_edge_index, len(sub_nodes),
                                                    type=self.hamilton_type)

                # è‡ªé€‚åº”æ¼”åŒ–æ—¶é—´
                adaptive_time = self.evolution_time * torch.sigmoid(self.evolution_scale)

                # è¶…ç¨³å®šçŸ©é˜µæŒ‡æ•°è®¡ç®—
                U_sub = ultra_stable_matrix_exp(H, adaptive_time)

                # æ„å»ºæ‰©å±•é…‰ç®—å­ï¼ˆä¿æŒæ ¸å¿ƒç‰¹æ€§ï¼‰
                dim = len(sub_nodes)
                U = torch.zeros(2 * dim, 2 * dim, dtype=torch.complex64, device=x.device)
                U[:dim, :dim] = U_sub
                U[dim:, dim:] = U_sub.conj().transpose(-2, -1)
                U = check_and_fix_nan_inf(U, f"extended_unitary_{i}")

                # åº”ç”¨é…‰æ¼”åŒ–
                z = torch.zeros(2 * dim, x.size(1), dtype=torch.complex64, device=x.device)
                z[:dim] = sub_x

                z_evolved = torch.matmul(U, z)
                z_evolved = check_and_fix_nan_inf(z_evolved, f"evolved_z_{i}")

                evolved[i] = z_evolved[center_idx]

            except Exception as e:
                print(f"âš ï¸ èŠ‚ç‚¹ {i} å¤„ç†å¤±è´¥: {e}ï¼Œä½¿ç”¨è¾“å…¥å€¼")
                evolved[i] = x[i]

        # æ¿€æ´»å‡½æ•°
        evolved = ultra_stable_complex_relu(evolved)

        # Dropout
        if self.training and self.dropout > 0:
            evolved = ultra_stable_complex_dropout(evolved, self.dropout, self.training)

        # å±‚å½’ä¸€åŒ–
        evolved = self.norm(evolved)

        # æ®‹å·®è¿æ¥
        if self.residual_proj is not None:
            if x_residual.is_floating_point():
                x_residual = torch.complex(x_residual, torch.zeros_like(x_residual))
            x_residual = self.residual_proj(x_residual)
        elif x_residual.is_floating_point():
            x_residual = torch.complex(x_residual, torch.zeros_like(x_residual))

        x_residual = check_and_fix_nan_inf(x_residual, "residual")
        result = evolved + x_residual * 0.1  # è¾ƒå°çš„æ®‹å·®æƒé‡

        return check_and_fix_nan_inf(result, "conv_output")


# ==== ä¸»ç½‘ç»œ ====
class UltraStableOptimizedLocalUnitaryGCN(nn.Module):
    def __init__(self, input_dim, hidden_dims, output_dim,
                 k_hop=1, evolution_times=None, hamilton_type='laplacian',
                 dropout=0.05, max_subgraph_size=4):
        super().__init__()
        dims = [input_dim] + hidden_dims + [output_dim]
        self.layers = nn.ModuleList()

        if evolution_times is None:
            # æ›´ä¿å®ˆçš„æ¼”åŒ–æ—¶é—´
            evolution_times = [0.05, 0.08, 0.1][:len(dims) - 1]
            evolution_times = evolution_times + [evolution_times[-1]] * (len(dims) - 1 - len(evolution_times))

        for i in range(len(dims) - 1):
            self.layers.append(UltraStableLocalUnitaryGCNConv(
                in_channels=dims[i],
                out_channels=dims[i + 1],
                k_hop=k_hop,
                evolution_time=evolution_times[i],
                hamilton_type=hamilton_type,
                dropout=dropout if i < len(dims) - 2 else 0.0,
                max_subgraph_size=max_subgraph_size
            ))

        # æ›´å¼ºçš„æ¢¯åº¦è£å‰ª
        self.grad_clip = 0.5

        # è¾“å‡ºç¨³å®šåŒ–
        self.output_scale = nn.Parameter(torch.tensor(0.1))

    def forward(self, data):
        x, edge_index, batch = data.x, data.edge_index, data.batch

        # è¾“å…¥å½’ä¸€åŒ–
        x = F.normalize(x, p=2, dim=1) * 0.1  # å°å¹…åˆå§‹åŒ–

        for i, layer in enumerate(self.layers):
            x_prev = x
            x = layer(x, edge_index)

            # æ¯å±‚åæ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
            x = check_and_fix_nan_inf(x, f"layer_{i}_output")

            # æ¸è¿›å¼æ¢¯åº¦è£å‰ª
            if self.training:
                torch.nn.utils.clip_grad_norm_(self.parameters(), self.grad_clip)

            # æ£€æŸ¥æ˜¯å¦æœ‰é—®é¢˜ï¼Œå¦‚æœæœ‰åˆ™è·³è¿‡è¯¥å±‚
            if torch.isnan(x).any() or torch.isinf(x).any():
                print(f"âš ï¸ ç¬¬{i}å±‚è¾“å‡ºå¼‚å¸¸ï¼Œä½¿ç”¨å‰ä¸€å±‚è¾“å‡º")
                x = x_prev

        # æ¨¡é•¿åˆ†ç±»ï¼ˆæ ¸å¿ƒç‰¹æ€§ä¿æŒï¼‰
        x = safe_complex_magnitude(x)
        x = x * torch.sigmoid(self.output_scale)  # è‡ªé€‚åº”è¾“å‡ºç¼©æ”¾

        # å…¨å±€å¹³å‡æ± åŒ–
        x = global_mean_pool(x, batch)
        x = check_and_fix_nan_inf(x, "pooled_output")

        # æœ€ç»ˆè¾“å‡ºç¨³å®šåŒ–
        x = torch.clamp(x, min=1e-6, max=10.0)

        return F.log_softmax(x, dim=1)


# ==== æµ‹è¯•å‡½æ•° ====
def test_ultra_stable_local_unitary_gcn():
    print("ğŸ›¡ï¸ è¶…ç¨³å®šç‰ˆ LocalUnitaryGCN æµ‹è¯•ï¼ˆä¿®å¤NaNé—®é¢˜ï¼‰")
    print("=" * 70)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # æ¨¡æ‹Ÿå›¾æ„å»º - æ›´å°æ›´ç¨³å®š
    num_nodes = 50
    num_features = 32
    num_classes = 2
    edge_index = erdos_renyi_graph(num_nodes, edge_prob=0.03).to(device)
    x = torch.randn(num_nodes, num_features).to(device) * 0.01  # å¾ˆå°çš„åˆå§‹å€¼
    batch = torch.zeros(num_nodes, dtype=torch.long).to(device)
    data = Data(x=x, edge_index=edge_index, batch=batch)

    print(f"\nğŸ“Š å›¾ä¿¡æ¯: {num_nodes} èŠ‚ç‚¹, {edge_index.size(1)} æ¡è¾¹")

    model = UltraStableOptimizedLocalUnitaryGCN(
        input_dim=num_features,
        hidden_dims=[16, 8],
        output_dim=num_classes,
        k_hop=1,
        evolution_times=[1, 1, 1],  # éå¸¸å°çš„æ¼”åŒ–æ—¶é—´
        hamilton_type='laplacian',
        dropout=0.5,  # å¾ˆå°çš„dropout
        max_subgraph_size=3  # å¾ˆå°çš„å­å›¾
    ).to(device)

    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    # æ¨ç†æµ‹è¯•
    model.eval()
    with torch.no_grad():
        try:
            output = model(data)
            print(f"âœ… è¾“å‡ºå½¢çŠ¶: {output.shape}")
            print(f"âœ… è¾“å‡ºèŒƒå›´: [{output.min().item():.6f}, {output.max().item():.6f}]")
            print(f"âœ… è¾“å‡ºæ— NaN: {not torch.isnan(output).any().item()}")
            print(f"âœ… è¾“å‡ºæ— Inf: {not torch.isinf(output).any().item()}")

            # æ£€æŸ¥è¾“å‡ºåˆ†å¸ƒ
            print(f"âœ… è¾“å‡ºæ¦‚ç‡å’Œ: {torch.exp(output).sum(dim=1)}")

        except Exception as e:
            print(f"âŒ æ¨¡å‹æ‰§è¡Œå‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return

    # æ¢¯åº¦æµ‹è¯• - ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡
    model.train()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-6)

    print("\nğŸ”„ å¼€å§‹è®­ç»ƒæµ‹è¯•...")
    for epoch in range(5):
        try:
            optimizer.zero_grad()
            y_fake = torch.randint(0, num_classes, (1,)).to(device)
            output = model(data)

            # æ£€æŸ¥è¾“å‡º
            if torch.isnan(output).any() or torch.isinf(output).any():
                print(f"âŒ Epoch {epoch}: è¾“å‡ºåŒ…å«NaN/Inf")
                continue

            loss = F.nll_loss(output, y_fake)

            if torch.isnan(loss) or torch.isinf(loss):
                print(f"âŒ Epoch {epoch}: æŸå¤±ä¸ºNaN/Inf")
                continue

            print(f"âœ… Epoch {epoch}: Loss = {loss.item():.6f}")

            loss.backward()
            # æ£€æŸ¥æ¢¯åº¦
            total_grad_norm = 0
            nan_grad_count = 0
            for name, param in model.named_parameters():
                if param.grad is not None:
                    grad_norm = param.grad.data.norm(2).item()
                    if math.isnan(grad_norm) or math.isinf(grad_norm):
                        nan_grad_count += 1
                        print(f"âš ï¸ {name} æ¢¯åº¦å¼‚å¸¸")
                        param.grad.data.fill_(1e-6)
                    else:
                        total_grad_norm += grad_norm ** 2

            total_grad_norm = total_grad_norm ** 0.5
            print(f"âœ… æ€»æ¢¯åº¦èŒƒæ•°: {total_grad_norm:.6f}, NaNæ¢¯åº¦æ•°: {nan_grad_count}")

            optimizer.step()

        except Exception as e:
            print(f"âŒ Epoch {epoch} è®­ç»ƒå‡ºé”™: {e}")
            import traceback
            traceback.print_exc()

    print("\nğŸ¯ æ ¸å¿ƒç‰¹æ€§ä¿æŒç¡®è®¤:")
    print("  âœ… å®Œæ•´å¤æ•°æ¼”åŒ– - æ‰€æœ‰è®¡ç®—åœ¨å¤æ•°åŸŸè¿›è¡Œ")
    print("  âœ… æ¨¡é•¿åˆ†ç±» - æœ€ç»ˆè¾“å‡ºä½¿ç”¨å¤æ•°æ¨¡é•¿")
    print("  âœ… å±€éƒ¨å­å›¾é‚»æ¥ç”Ÿæˆé…‰çŸ©é˜µ - å“ˆå¯†é¡¿çŸ©é˜µåŸºäºå­å›¾æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ")
    print("  âœ… é…‰æ‰©å¼ ä¿è¯å¹ºæ­£ç‰¹æ€§ - 2nÃ—2næ‰©å±•é…‰ç®—å­")
    print("  âœ… æ•°å€¼ç¨³å®šæ€§å¤§å¹…æå‡ - å¤šé‡æ£€æŸ¥å’Œä¿®å¤æœºåˆ¶")


if __name__ == "__main__":
    test_ultra_stable_local_unitary_gcn()