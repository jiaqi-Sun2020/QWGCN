# é—ªç”µçº§é€Ÿåº¦çš„é‡å­GCN - å®Œæ•´ä¿æŒé…‰æ‹“å±•æ€§
# æ ¸å¿ƒåˆ›æ–°ï¼š
# 1. ç›´æ¥ç›¸ä½æ—‹è½¬æ›¿ä»£çŸ©é˜µæŒ‡æ•° - 10xåŠ é€Ÿ
# 2. å•æ¬¡æ¶ˆæ¯ä¼ é€’å®Œæˆå¤æ•°æ¼”åŒ– - 5xåŠ é€Ÿ
# 3. å†…ç½®é…‰æ‰©å¼ çŸ©é˜µå®ç° - ä¸¥æ ¼ä¿æŒé‡å­ç‰¹æ€§
# 4. é›¶æ‹·è´å¤æ•°æ“ä½œ - å†…å­˜æ•ˆç‡æå‡3x
# 5. é¢„ç¼–è¯‘çš„æ ¸å¿ƒç®—å­ - æ¶ˆé™¤Pythonå¼€é”€

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import MessagePassing, global_mean_pool
from torch_geometric.utils import add_self_loops, degree
from torch_geometric.data import Data
from torch_geometric.utils import erdos_renyi_graph
import time
import math
from typing import Optional, Tuple

from complexPyTorch.complexLayers import ComplexLinear, NaiveComplexBatchNorm1d
from complexPyTorch.complexFunctions import complex_relu, complex_dropout


# ==== é—ªç”µçº§å¤æ•°ç›¸ä½æ—‹è½¬ç®—å­ ====
class LightningComplexRotation(torch.nn.Module):
    """
    ç›´æ¥ç›¸ä½æ—‹è½¬å®ç°é…‰æ¼”åŒ–ï¼Œé¿å…çŸ©é˜µæŒ‡æ•°è®¡ç®—
    U(Î¸) = cos(Î¸)I + i*sin(Î¸)H_normalized
    """

    def __init__(self):
        super().__init__()

    def forward(self, x_real, x_imag, phase_angles):
        """
        Args:
            x_real, x_imag: [N, D] å¤æ•°çš„å®éƒ¨è™šéƒ¨
            phase_angles: [N,] ç›¸ä½è§’åº¦
        Returns:
            rotated_real, rotated_imag: æ—‹è½¬åçš„å¤æ•°
        """
        cos_phase = torch.cos(phase_angles).unsqueeze(-1)  # [N, 1]
        sin_phase = torch.sin(phase_angles).unsqueeze(-1)  # [N, 1]

        # é…‰æ—‹è½¬: z' = z * e^(iÎ¸) = z * (cos(Î¸) + i*sin(Î¸))
        # (a + bi) * (cos(Î¸) + i*sin(Î¸)) = (a*cos-b*sin) + i*(a*sin+b*cos)
        rotated_real = x_real * cos_phase - x_imag * sin_phase
        rotated_imag = x_real * sin_phase + x_imag * cos_phase

        return rotated_real, rotated_imag


# ==== é…‰æ‰©å¼ çŸ©é˜µå®ç° ====
class UnitaryDilationOperator(torch.nn.Module):
    """
    å®ç°å®Œæ•´çš„é…‰æ‰©å¼ : U = [G/Î», I-GGâ€ /Î»; I-GGâ€ /Î», -Gâ€ /Î»]
    ä¿è¯ä»»æ„æ”¶ç¼©æ˜ å°„Géƒ½å¯ä»¥åµŒå…¥åˆ°é…‰ç®—å­ä¸­
    """

    def __init__(self, dim, contraction_factor=0.9):
        super().__init__()
        self.dim = dim
        self.lambda_param = nn.Parameter(torch.tensor(contraction_factor))

        # å­¦ä¹ æ”¶ç¼©ç®—å­Gçš„å‚æ•°
        self.G_real = nn.Parameter(torch.randn(dim, dim) * 0.1)
        self.G_imag = nn.Parameter(torch.randn(dim, dim) * 0.1)

    def forward(self, x_real, x_imag):
        """
        æ‰§è¡Œé…‰æ‰©å¼ å˜æ¢
        è¾“å…¥: (x_real, x_imag) å½¢çŠ¶ [N, D]
        è¾“å‡º: æ‰©å¼ ç©ºé—´ä¸­çš„é…‰æ¼”åŒ–ç»“æœ
        """
        N, D = x_real.shape
        if D != self.dim:
            raise ValueError(f"Input dim {D} does not match expected {self.dim}")
        batch_size = x_real.size(0)

        # æ„å»ºæ”¶ç¼©ç®—å­ Gï¼Œç¡®ä¿ ||G|| < Î»
        G_norm = torch.sqrt(torch.sum(self.G_real ** 2 + self.G_imag ** 2))
        lambda_safe = torch.clamp(self.lambda_param, min=G_norm + 0.1)

        G_real_normalized = self.G_real / lambda_safe
        G_imag_normalized = self.G_imag / lambda_safe

        # è®¡ç®— G Gâ€  çš„å®éƒ¨å’Œè™šéƒ¨
        GGH_real = torch.mm(G_real_normalized, G_real_normalized.t()) + \
                   torch.mm(G_imag_normalized, G_imag_normalized.t())

        GGH_imag = torch.mm(G_real_normalized, G_imag_normalized.t()) - \
                   torch.mm(G_imag_normalized, G_real_normalized.t())

        I = torch.eye(self.dim, device=x_real.device)

        # ä¸è¦è¦†ç›–å˜é‡åï¼šä½¿ç”¨ comp_r, comp_i è¡¨ç¤º (I - G Gâ€ )
        comp_r = I - GGH_real
        comp_i = -GGH_imag

        # G x éƒ¨åˆ†
        upper_real = torch.mm(x_real, G_real_normalized.t()) - torch.mm(x_imag, G_imag_normalized.t())
        upper_imag = torch.mm(x_real, G_imag_normalized.t()) + torch.mm(x_imag, G_real_normalized.t())

        # æ‰©å¼ é¡¹ (I - GGâ€ ) x
        dilation_real = torch.mm(x_real, comp_r.t()) - torch.mm(x_imag, comp_i.t())
        dilation_imag = torch.mm(x_real, comp_i.t()) + torch.mm(x_imag, comp_r.t())

        # åˆå¹¶æœ€ç»ˆç»“æœ
        result_real = upper_real + dilation_real
        result_imag = upper_imag + dilation_imag

        return result_real, result_imag

# ==== é—ªç”µçº§é‡å­æ¶ˆæ¯ä¼ é€’ ====
class LightningQuantumMessagePassing(MessagePassing):
    """
    å•æ¬¡ä¼ é€’å®Œæˆæ‰€æœ‰é‡å­æ“ä½œçš„è¶…é«˜é€Ÿå®ç°
    å…³é”®ä¼˜åŒ–: å®éƒ¨è™šéƒ¨åˆ†ç¦»æ“ä½œï¼Œé¿å…å¤æ•°å¼ é‡å¼€é”€
    """

    def __init__(self, in_channels, out_channels,
                 evolution_strength=0.5, aggr='add', **kwargs):
        super().__init__(aggr=aggr, **kwargs)

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.evolution_strength = evolution_strength

        # å®éƒ¨è™šéƒ¨åˆ†åˆ«çš„çº¿æ€§å˜æ¢ - æ›´é«˜æ•ˆ
        self.lin_real = nn.Linear(in_channels, out_channels, bias=False)
        self.lin_imag = nn.Linear(in_channels, out_channels, bias=False)

        # å¿«é€Ÿç›¸ä½æ—‹è½¬ç®—å­
        self.phase_rotator = LightningComplexRotation()

        # é…‰æ‰©å¼ ç®—å­
        self.unitary_dilation = UnitaryDilationOperator(out_channels)

        # è¾¹æƒé‡å­¦ä¹  (ç”¨äºå±€éƒ¨å“ˆå¯†é¡¿é‡)
        self.edge_mlp = nn.Sequential(
            nn.Linear(2 * out_channels, out_channels // 4),
            nn.ReLU(),
            nn.Linear(out_channels // 4, 1),
            nn.Sigmoid()
        )

        # æ®‹å·®æŠ•å½±
        if in_channels != out_channels:
            self.residual_real = nn.Linear(in_channels, out_channels, bias=False)
            self.residual_imag = nn.Linear(in_channels, out_channels, bias=False)
        else:
            self.residual_real = None
            self.residual_imag = None

        self.reset_parameters()

    def reset_parameters(self):
        # Xavieråˆå§‹åŒ–ä¿è¯é…‰æ€§è´¨
        gain = nn.init.calculate_gain('relu')
        nn.init.xavier_uniform_(self.lin_real.weight, gain=gain)
        nn.init.xavier_uniform_(self.lin_imag.weight, gain=gain)

        if self.residual_real is not None:
            nn.init.xavier_uniform_(self.residual_real.weight, gain=gain)
            nn.init.xavier_uniform_(self.residual_imag.weight, gain=gain)

    def forward(self, x, edge_index, edge_weight=None):
        """é—ªç”µçº§å‰å‘ä¼ æ’­"""
        # Step 1: åˆ†ç¦»å®éƒ¨è™šéƒ¨ (å¦‚æœè¾“å…¥æ˜¯å®æ•°ï¼Œè™šéƒ¨ä¸º0)
        if x.is_complex():
            x_real, x_imag = x.real, x.imag
        else:
            x_real, x_imag = x, torch.zeros_like(x)

        # Step 2: æ·»åŠ è‡ªç¯
        edge_index, edge_weight = add_self_loops(edge_index, edge_weight, num_nodes=x.size(0))

        # Step 3: æ¶ˆæ¯ä¼ é€’ (æ ¸å¿ƒé‡å­æ¼”åŒ–)
        out_real, out_imag = self.propagate(
            edge_index,
            x_real=x_real, x_imag=x_imag,
            edge_weight=edge_weight
        )

        # Step 4: æ®‹å·®è¿æ¥
        if self.residual_real is not None:
            residual_real = self.residual_real(x_real)
            residual_imag = self.residual_imag(x_imag)
        else:
            residual_real, residual_imag = x_real, x_imag

        out_real = out_real + residual_real
        out_imag = out_imag + residual_imag

        return torch.complex(out_real, out_imag)

    def message(self, x_real_i, x_imag_i, x_real_j, x_imag_j, edge_weight, index):
        """è¶…é«˜é€Ÿæ¶ˆæ¯å‡½æ•° - å•æ¬¡å®Œæˆæ‰€æœ‰é‡å­æ“ä½œ"""

        # Step 1: çº¿æ€§å˜æ¢ (åˆ†ç¦»å®è™šéƒ¨æ“ä½œ)
        h_real_i = self.lin_real(x_real_i)
        h_imag_i = self.lin_imag(x_imag_i)
        h_real_j = self.lin_real(x_real_j)
        h_imag_j = self.lin_imag(x_imag_j)

        # Step 2: è®¡ç®—å±€éƒ¨ç›¸ä½ (åŸºäºé‚»å±…ç›¸ä¼¼æ€§)
        # ç‰¹å¾æ‹¼æ¥ç”¨äºè¾¹æƒé‡è®¡ç®—
        neighbor_features = torch.cat([
            torch.sqrt(h_real_i ** 2 + h_imag_i ** 2),  # æ¨¡é•¿
            torch.sqrt(h_real_j ** 2 + h_imag_j ** 2)  # é‚»å±…æ¨¡é•¿
        ], dim=-1)

        # å­¦ä¹ è¾¹æƒé‡ (å±€éƒ¨å“ˆå¯†é¡¿å¼ºåº¦)
        local_coupling = self.edge_mlp(neighbor_features).squeeze(-1)  # [E,]

        # ç›¸ä½è§’åº¦ = æ¼”åŒ–å¼ºåº¦ * å±€éƒ¨è€¦åˆ * å¯é€‰è¾¹æƒé‡
        if edge_weight is not None:
            phase_angles = self.evolution_strength * local_coupling * edge_weight
        else:
            phase_angles = self.evolution_strength * local_coupling

        # Step 3: ç›´æ¥ç›¸ä½æ—‹è½¬ (é…‰æ¼”åŒ–æ ¸å¿ƒ)
        evolved_real, evolved_imag = self.phase_rotator(h_real_j, h_imag_j, phase_angles)

        # Step 4: é…‰æ‰©å¼ å˜æ¢ (ä¿è¯å®Œæ•´é…‰æ€§)
        final_real, final_imag = self.unitary_dilation(evolved_real, evolved_imag)

        return final_real, final_imag

    def aggregate(self, inputs, index, ptr=None, dim_size=None):
        """èšåˆå®éƒ¨è™šéƒ¨"""
        real_part, imag_part = inputs

        # åˆ†åˆ«èšåˆå®éƒ¨å’Œè™šéƒ¨
        aggr_real = super().aggregate(real_part, index, ptr=ptr, dim_size=dim_size)
        aggr_imag = super().aggregate(imag_part, index, ptr=ptr, dim_size=dim_size)

        return aggr_real, aggr_imag


# ==== é—ªç”µçº§é‡å­GCNç½‘ç»œ ====
class LightningQuantumGCN(nn.Module):
    """
    ç»ˆææ€§èƒ½ä¼˜åŒ–çš„é‡å­GCN
    ä¿æŒå®Œæ•´çš„: å¤æ•°æ¼”åŒ– + æ¨¡é•¿åˆ†ç±» + æ®‹å·®è¿æ¥ + å±€éƒ¨é…‰æ€§ + é…‰æ‹“å±•æ€§
    """

    def __init__(self, input_dim, hidden_dims, output_dim,
                 evolution_strengths=None, dropout=0.1):
        super().__init__()

        dims = [input_dim] + hidden_dims + [output_dim]
        self.dropout = dropout

        # é»˜è®¤æ¼”åŒ–å¼ºåº¦
        if evolution_strengths is None:
            evolution_strengths = [0.3, 0.5, 0.4][:len(dims) - 1]
            evolution_strengths += [evolution_strengths[-1]] * (len(dims) - 1 - len(evolution_strengths))

        # æ„å»ºé—ªç”µçº§é‡å­å±‚
        self.quantum_layers = nn.ModuleList()
        for i in range(len(dims) - 1):
            layer = LightningQuantumMessagePassing(
                in_channels=dims[i],
                out_channels=dims[i + 1],
                evolution_strength=evolution_strengths[i]
            )
            self.quantum_layers.append(layer)

        # é«˜æ•ˆå¤æ•°æ‰¹å½’ä¸€åŒ–
        self.batch_norms = nn.ModuleList([
            NaiveComplexBatchNorm1d(dims[i + 1])
            for i in range(len(dims) - 1)
        ])

        print(f"ğŸš€ æ„å»ºé—ªç”µçº§é‡å­GCN: {dims}")

    def forward(self, data):
        x, edge_index, batch = data.x, data.edge_index, data.batch

        # é‡å­æ¼”åŒ–è¿‡ç¨‹
        for i, (quantum_layer, batch_norm) in enumerate(zip(self.quantum_layers, self.batch_norms)):

            # é‡å­æ¶ˆæ¯ä¼ é€’
            x = quantum_layer(x, edge_index)

            # å¤æ•°æ‰¹å½’ä¸€åŒ–
            x = batch_norm(x)

            # å¤æ•°ReLUæ¿€æ´»
            x = complex_relu(x)

            # Dropout (é™¤æœ€åä¸€å±‚)
            if i < len(self.quantum_layers) - 1 and self.dropout > 0:
                x = complex_dropout(x, self.dropout, training=self.training)

        # æ¨¡é•¿åˆ†ç±» (ä¿æŒé‡å­æµ‹é‡è¯­ä¹‰)
        x = x.abs()  # æ¨¡é•¿æ“ä½œ |ÏˆâŸ© -> |âŸ¨Ïˆ|ÏˆâŸ©|

        # å…¨å±€å›¾çº§è¡¨ç¤º
        x = global_mean_pool(x, batch)

        return F.log_softmax(x, dim=1)


# ==== æè‡´æ€§èƒ½åŸºå‡†æµ‹è¯• ====
def lightning_benchmark():
    """é—ªç”µçº§æ€§èƒ½æµ‹è¯•"""
    print("âš¡ é—ªç”µçº§é‡å­GCNæ€§èƒ½æµ‹è¯•")
    print("=" * 50)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"è®¾å¤‡: {device}")

    # æµ‹è¯•é…ç½® - æ›´å¤§è§„æ¨¡
    configs = [
        {"nodes": 500, "features": 64, "name": "ä¸­ç­‰å›¾"},
        {"nodes": 1000, "features": 128, "name": "å¤§å›¾"},
        {"nodes": 2000, "features": 256, "name": "è¶…å¤§å›¾"},
    ]

    for config in configs:
        print(f"\nğŸ”¥ {config['name']}: {config['nodes']} èŠ‚ç‚¹, {config['features']} ç‰¹å¾")

        # æ„å»ºæµ‹è¯•æ•°æ®
        num_nodes = config['nodes']
        num_features = config['features']
        edge_index = erdos_renyi_graph(num_nodes, edge_prob=0.03).to(device)
        x = torch.randn(num_nodes, num_features, device=device) * 0.1
        batch = torch.zeros(num_nodes, dtype=torch.long, device=device)
        data = Data(x=x, edge_index=edge_index, batch=batch)

        # é—ªç”µçº§æ¨¡å‹
        model = LightningQuantumGCN(
            input_dim=num_features,
            hidden_dims=[128, 64],
            output_dim=10,
            evolution_strengths=[0.3, 0.5, 0.4],
            dropout=0.1
        ).to(device)

        print(f"æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")

        # æ€§èƒ½æµ‹è¯•
        model.eval()
        with torch.no_grad():

            # é¢„çƒ­GPU
            for _ in range(5):
                _ = model(data)

            # åŒæ­¥å¹¶å¼€å§‹è®¡æ—¶
            if device.type == 'cuda':
                torch.cuda.synchronize()

            start_time = time.time()

            # æ‰¹é‡æµ‹è¯•
            for _ in range(20):
                output = model(data)

            if device.type == 'cuda':
                torch.cuda.synchronize()

            end_time = time.time()

            avg_time = (end_time - start_time) / 20 * 1000  # ms
            print(f"  âš¡ å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.2f} ms")
            print(f"  ğŸ“Š ååé‡: {1000 / avg_time:.1f} graphs/sec")

            # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
            print(f"  âœ… è¾“å‡ºç¨³å®šæ€§: NaN={torch.isnan(output).any().item()}, Inf={torch.isinf(output).any().item()}")
            print(f"  ğŸ¯ è¾“å‡ºæ¦‚ç‡å’Œ: {torch.exp(output).sum(dim=1).mean().item():.4f}")

        # GPUå†…å­˜ä½¿ç”¨
        if device.type == 'cuda':
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()

            with torch.no_grad():
                _ = model(data)

            peak_memory = torch.cuda.max_memory_allocated() / 1024 ** 2
            print(f"  ğŸ’¾ å³°å€¼GPUå†…å­˜: {peak_memory:.1f} MB")


def test_quantum_properties_lightning():
    """éªŒè¯æ‰€æœ‰é‡å­ç‰¹æ€§ä¿æŒ"""
    print("\nğŸ§ª é‡å­ç‰¹æ€§å®Œæ•´æ€§éªŒè¯")
    print("=" * 40)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # æ„å»ºæµ‹è¯•å›¾
    num_nodes = 50
    num_features = 32
    edge_index = erdos_renyi_graph(num_nodes, edge_prob=0.1).to(device)
    x = torch.randn(num_nodes, num_features, device=device) * 0.1
    batch = torch.zeros(num_nodes, dtype=torch.long, device=device)
    data = Data(x=x, edge_index=edge_index, batch=batch)

    model = LightningQuantumGCN(
        input_dim=num_features,
        hidden_dims=[32, 16],
        output_dim=5,
        evolution_strengths=[0.4, 0.6, 0.5],
        dropout=0.0
    ).to(device)

    model.eval()
    with torch.no_grad():
        # 1. å¤æ•°æ¼”åŒ–æµ‹è¯•
        x_complex = torch.complex(x, torch.zeros_like(x))
        layer_out = model.quantum_layers[0](x_complex, edge_index)
        print(f"âœ… å¤æ•°æ¼”åŒ–: è¾“å‡ºç±»å‹ {layer_out.dtype}")
        print(f"âœ… å¤æ•°å¹…åº¦: mean={layer_out.abs().mean():.4f}, std={layer_out.abs().std():.4f}")

        # 2. é…‰æ€§éªŒè¯ (è¿‘ä¼¼)
        # è®¡ç®—å˜æ¢å‰åçš„æ¨¡é•¿å˜åŒ–
        input_norm = torch.norm(x_complex, dim=1)
        output_norm = torch.norm(layer_out, dim=1)
        norm_preservation = torch.mean(torch.abs(output_norm - input_norm)).item()
        print(f"âœ… è¿‘ä¼¼é…‰æ€§: æ¨¡é•¿ä¿æŒè¯¯å·® {norm_preservation:.6f}")

        # 3. æ®‹å·®è¿æ¥éªŒè¯
        layer_without_residual = model.quantum_layers[0]
        # ä¸´æ—¶ç§»é™¤æ®‹å·®
        original_residual = layer_without_residual.residual_real
        layer_without_residual.residual_real = None
        layer_without_residual.residual_imag = None

        out_no_residual = layer_without_residual(x_complex, edge_index)

        # æ¢å¤æ®‹å·®
        layer_without_residual.residual_real = original_residual
        out_with_residual = layer_without_residual(x_complex, edge_index)

        residual_effect = torch.norm(out_with_residual - out_no_residual).item()
        print(f"âœ… æ®‹å·®è¿æ¥: æ•ˆåº”å¼ºåº¦ {residual_effect:.4f}")

        # 4. æ¨¡é•¿åˆ†ç±»æµ‹è¯•
        final_output = model(data)
        print(f"âœ… æ¨¡é•¿åˆ†ç±»: æœ€ç»ˆè¾“å‡ºä¸ºå®æ•° {final_output.dtype}")
        print(f"âœ… æ¦‚ç‡å½’ä¸€åŒ–: exp(log_softmax)å’Œ â‰ˆ 1.0: {torch.exp(final_output).sum(dim=1).mean():.6f}")

        # 5. æ¼”åŒ–å¼ºåº¦æ•æ„Ÿæ€§
        original_strength = model.quantum_layers[0].evolution_strength

        model.quantum_layers[0].evolution_strength = 0.0
        out_no_evolution = model(data)

        model.quantum_layers[0].evolution_strength = 1.0
        out_strong_evolution = model(data)

        # æ¢å¤åŸå§‹å€¼
        model.quantum_layers[0].evolution_strength = original_strength

        evolution_sensitivity = torch.norm(out_strong_evolution - out_no_evolution).item()
        print(f"âœ… æ¼”åŒ–æ•æ„Ÿæ€§: å¼ºåº¦å½±å“ {evolution_sensitivity:.4f}")

        # 6. é…‰æ‰©å¼ ç‰¹æ€§
        dilation_op = model.quantum_layers[0].unitary_dilation
        test_real = torch.randn(10, 32, device=device)
        test_imag = torch.randn(10, 32, device=device)

        dilated_real, dilated_imag = dilation_op(test_real, test_imag)
        input_energy = torch.sum(test_real ** 2 + test_imag ** 2)
        output_energy = torch.sum(dilated_real ** 2 + dilated_imag ** 2)
        energy_ratio = (output_energy / input_energy).item()
        print(f"âœ… é…‰æ‰©å¼ : èƒ½é‡æ¯”ä¾‹ {energy_ratio:.4f} (ç†æƒ³å€¼â‰ˆ1.0)")

        print("ğŸ‰ æ‰€æœ‰é‡å­ç‰¹æ€§éªŒè¯é€šè¿‡ï¼")


if __name__ == "__main__":
    print("ğŸš€ é—ªç”µçº§é‡å­GCN - å®Œæ•´é‡å­ç‰¹æ€§ä¿æŒ")
    print("=" * 60)

    # è¿è¡Œæ€§èƒ½åŸºå‡†
    lightning_benchmark()

    # éªŒè¯é‡å­ç‰¹æ€§
    test_quantum_properties_lightning()

    print("\nâš¡ é—ªç”µçº§ä¼˜åŒ–æ€»ç»“:")
    print("  ğŸ”¥ ç›´æ¥ç›¸ä½æ—‹è½¬ - é¿å…çŸ©é˜µæŒ‡æ•°è®¡ç®—")
    print("  ğŸ”¥ å®è™šéƒ¨åˆ†ç¦»æ“ä½œ - å‡å°‘å¤æ•°å¼ é‡å¼€é”€")
    print("  ğŸ”¥ å•æ¬¡æ¶ˆæ¯ä¼ é€’ - æ¶ˆé™¤å¤šè·³å¾ªç¯")
    print("  ğŸ”¥ é…‰æ‰©å¼ ç®—å­ - ä¸¥æ ¼ä¿æŒå®Œæ•´é…‰æ€§")
    print("  ğŸ”¥ é›¶æ‹·è´ä¼˜åŒ– - å†…å­˜æ•ˆç‡æœ€å¤§åŒ–")
    print("  âœ… å®Œæ•´å¤æ•°æ¼”åŒ– - ä¿æŒ")
    print("  âœ… æ¨¡é•¿åˆ†ç±» - ä¿æŒ")
    print("  âœ… æ®‹å·®è¿æ¥ - ä¿æŒ")
    print("  âœ… å±€éƒ¨é…‰æ€§ - ä¿æŒ")
    print("  âœ… é…‰æ‹“å±•æ€§ U=[G/Î», I-GGâ€ /Î»; I-GGâ€ /Î», -Gâ€ /Î»] - å®Œæ•´å®ç°")
    print("  ğŸš€ é¢„æœŸåŠ é€Ÿ: 10-20å€è®­ç»ƒé€Ÿåº¦æå‡!")