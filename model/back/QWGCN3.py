import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import MessagePassing
from torch_geometric.nn import global_mean_pool, global_max_pool
from torch_geometric.utils import degree
from torch_sparse import SparseTensor
import math
from torch_geometric.nn import GCNConv ,TopKPooling
import time
from torch.cuda.amp import autocast

from complexPyTorch.complexLayers import ComplexLinear, NaiveComplexBatchNorm1d
from complexPyTorch.complexFunctions import complex_relu, complex_dropout

from torch_geometric.loader import DataLoader
from torch_geometric.data import Data
from torch_geometric.utils import erdos_renyi_graph
""""
åœ¨GCNå±‚åé¢åŠ ä¸Šé‡å­æ¼”åŒ–å±‚ï¼ä¼˜åŒ–äº†é…‰çŸ©é˜µè¿ç®—å‡½æ•°apply_local_unitary_evolution
"""

def print_unitarity_error(U: torch.Tensor):
    """æ‰“å°ä¸€ä¸ªå¤æ•°çŸ©é˜µçš„é…‰æ€§è¯¯å·® â€–Uâ€ U - Iâ€–_F"""
    if U.dtype not in (torch.cfloat, torch.cdouble):
        raise ValueError("è¾“å…¥çŸ©é˜µå¿…é¡»æ˜¯å¤æ•°ç±»å‹")

    UH_U = U.conj().T @ U
    I = torch.eye(U.shape[0], dtype=U.dtype, device=U.device)
    error = torch.norm(UH_U - I, p='fro').item()
    print(f"ğŸ§ª é…‰æ€§è¯¯å·® â€–Uâ€ U - Iâ€–_F = {error:.6e}")


class ImprovedUnitaryDilationNetwork(nn.Module):
    """æ”¹è¿›çš„é…‰æ‰©å¼ ç½‘ç»œ"""

    def __init__(self, hidden_dim=64):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.lambda_logit = nn.Parameter(torch.logit(torch.tensor(0.8)))

        # å›¾ç¼–ç å™¨
        self.graph_encoder = nn.ModuleList([
            GCNConv(1, hidden_dim),
            GCNConv(hidden_dim, hidden_dim)
        ])

        self.method = 'antihermitian'
        # é¢„æ„å»ºä¸åŒå¤§å°çš„é¢„æµ‹å™¨ç¼“å­˜
        self.predictor_cache = {}

    def _build_antihermitian_predictor(self, N):
        """ä¸ºå½“å‰å­å›¾æ„å»ºåå„ç±³çŸ©é˜µé¢„æµ‹å™¨"""
        if N in self.predictor_cache:
            return self.predictor_cache[N]

        num_params = N * N
        predictor = nn.Sequential(
            nn.Linear(self.hidden_dim, self.hidden_dim),
            nn.ReLU(),
            nn.Linear(self.hidden_dim, self.hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(self.hidden_dim // 2, num_params),
            nn.Tanh()
        )
        self.predictor_cache[N] = predictor
        return predictor

    def encode_graph(self, edge_index):
        """ç¼–ç å›¾ç»“æ„"""
        device = edge_index.device
        N = int(edge_index.max()) + 1
        x = torch.ones(N, 1, device=device)

        for i, layer in enumerate(self.graph_encoder):
            x = layer(x, edge_index)
            if i < len(self.graph_encoder) - 1:
                x = torch.relu(x)

        batch = torch.zeros(N, dtype=torch.long, device=device)
        return global_mean_pool(x, batch).squeeze(0), N

    def construct_antihermitian_matrix(self, params, N):
        """ä»å‚æ•°æ„é€ åå„ç±³çŸ©é˜µ"""
        param_matrix = params.view(N, N)
        real_part = (param_matrix - param_matrix.T) / 2
        imag_part = (param_matrix + param_matrix.T) / 2
        return real_part + 1j * imag_part

    def proper_unitary_dilation(self, U_small):
        """æ­£ç¡®çš„é…‰æ‰©å¼ æ–¹æ³•"""
        device = U_small.device
        n = U_small.shape[0]

        UH_U = torch.conj(U_small).T @ U_small
        I = torch.eye(n, dtype=torch.cfloat, device=device)

        complement = I - UH_U
        complement = (complement + torch.conj(complement).T) / 2

        UU_H = U_small @ torch.conj(U_small).T
        complement2 = I - UU_H
        complement2 = (complement2 + torch.conj(complement2).T) / 2

        # æ•°å€¼ç¨³å®šçš„å¹³æ–¹æ ¹è®¡ç®—
        try:
            eigenvals, eigenvecs = torch.linalg.eigh(complement)
            eigenvals = torch.clamp(eigenvals.real, min=0.0)
            sqrt_complement = eigenvecs @ torch.diag(torch.sqrt(eigenvals + 1e-8)) @ torch.conj(eigenvecs).T
        except:
            # å›é€€æ–¹æ¡ˆ
            sqrt_complement = complement

        try:
            eigenvals2, eigenvecs2 = torch.linalg.eigh(complement2)
            eigenvals2 = torch.clamp(eigenvals2.real, min=0.0)
            sqrt_complement2 = eigenvecs2 @ torch.diag(torch.sqrt(eigenvals2 + 1e-8)) @ torch.conj(eigenvecs2).T
        except:
            sqrt_complement2 = complement2

        top = torch.cat([U_small, sqrt_complement], dim=1)
        bottom = torch.cat([sqrt_complement2, -torch.conj(U_small).T], dim=1)
        U_dilation = torch.cat([top, bottom], dim=0)

        return U_dilation

    def forward(self, edge_index):
        device = edge_index.device
        graph_emb, N = self.encode_graph(edge_index)

        if self.method == 'antihermitian':
            predictor = self._build_antihermitian_predictor(N).to(graph_emb.device)
            params = predictor(graph_emb)
            A = self.construct_antihermitian_matrix(params, N)
            A = A * 0.05  # å‡å°ç¼©æ”¾å› å­æé«˜æ•°å€¼ç¨³å®šæ€§
            U_small = torch.matrix_exp(A)
            U_dilation = self.proper_unitary_dilation(U_small)

        return U_dilation, U_small


class LocalQuantumLayer(MessagePassing):
    """å®Œæ•´çš„å±€éƒ¨é‡å­æ¼”åŒ–å±‚"""

    def __init__(self, input_dim, output_dim, dropout=0.1):
        super().__init__(aggr='add')
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.dropout = dropout

        # é…‰æ‰©å¼ ç½‘ç»œ
        self.unitary_network = ImprovedUnitaryDilationNetwork(hidden_dim=64)

        # å¤æ•°ç‰¹å¾å˜æ¢
        self.complex_real_transform = nn.Sequential(
            nn.Linear(input_dim, output_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        self.complex_imag_transform = nn.Sequential(
            nn.Linear(input_dim, output_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )

        # æ¶ˆæ¯èšåˆæƒé‡ï¼ˆç¡®ä¿å¤æ•°å…¼å®¹ï¼‰
        self.message_weights = nn.Parameter(torch.randn(output_dim, output_dim, dtype=torch.cfloat))

        # æ®‹å·®è¿æ¥çš„æŠ•å½±å±‚
        if input_dim != output_dim:
            self.residual_proj = nn.Linear(input_dim, output_dim)
        else:
            self.residual_proj = nn.Identity()

    def forward(self, x_real, x_imag, edge_index):
        """
        å‰å‘ä¼ æ’­
        Args:
            x_real: å®éƒ¨ç‰¹å¾ [N, input_dim]
            x_imag: è™šéƒ¨ç‰¹å¾ [N, input_dim]
            edge_index: è¾¹ç´¢å¼• [2, E]
        """
        device = x_real.device
        num_nodes = x_real.size(0)

        # 1. è·å–é…‰æ¼”åŒ–çŸ©é˜µï¼ˆåŸºäºå›¾ç»“æ„ï¼‰
        U_dilation, U_small = self.unitary_network(edge_index)

        # 2. ç‰¹å¾å˜æ¢åˆ°è¾“å‡ºç»´åº¦
        x_real_transformed = self.complex_real_transform(x_real)
        x_imag_transformed = self.complex_imag_transform(x_imag)

        # 3. æ„é€ å¤æ•°ç‰¹å¾çŸ©é˜µ
        x_complex = x_real_transformed + 1j * x_imag_transformed  # [N, output_dim]

        # 4. åº”ç”¨å±€éƒ¨é…‰æ¼”åŒ–ï¼ˆåªå¯¹è¿æ¥çš„èŠ‚ç‚¹è¿›è¡Œæ¼”åŒ–ï¼‰
        evolved_features = self.apply_local_unitary_evolution(
            x_complex, edge_index, U_dilation
        )

        # 5. æ¶ˆæ¯ä¼ é€’
        out_complex = self.propagate(edge_index, x=evolved_features)

        # 6. æ®‹å·®è¿æ¥
        residual_real = self.residual_proj(x_real)
        residual_imag = self.residual_proj(x_imag)
        residual_complex = residual_real + 1j * residual_imag

        out_complex = out_complex + residual_complex

        # 7. åˆ†ç¦»å®éƒ¨å’Œè™šéƒ¨
        out_real = out_complex.real
        out_imag = out_complex.imag

        return out_real, out_imag

    # è¿›ä¸€æ­¥ä¼˜åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨ç¨€ç–æ“ä½œå’Œé¢„è®¡ç®—
    def apply_local_unitary_evolution(self, x_complex, edge_index, U_dilation):
        """åœ¨å±€éƒ¨é‚»åŸŸåº”ç”¨é…‰æ¼”åŒ– - é«˜åº¦ä¼˜åŒ–ç‰ˆæœ¬"""
        device = x_complex.device
        num_nodes = x_complex.size(0)
        output_dim = x_complex.size(1)
        unitary_dim = U_dilation.size(0)

        # å¦‚æœé…‰çŸ©é˜µç»´åº¦å¤ªå°ï¼Œå›é€€åˆ°åŸå§‹æ–¹æ³•
        if unitary_dim < 2:
            return x_complex

        # ä½¿ç”¨torch_geometricçš„utilityæ„å»ºé‚»æ¥ä¿¡æ¯
        from torch_geometric.utils import to_undirected, degree

        # ç¡®ä¿è¾¹æ˜¯æ— å‘çš„
        edge_index_undirected = to_undirected(edge_index)

        # è®¡ç®—åº¦æ•°ï¼Œç­›é€‰æœ‰é‚»å±…çš„èŠ‚ç‚¹
        node_degrees = degree(edge_index_undirected[0], num_nodes=num_nodes)
        connected_nodes = torch.nonzero(node_degrees > 0).squeeze(-1)

        if len(connected_nodes) == 0:
            return x_complex

        # åˆå§‹åŒ–è¾“å‡º
        evolved_x = x_complex.clone()

        # æ‰¹é‡å¤„ç†ï¼šå°†è¿æ¥çš„èŠ‚ç‚¹åˆ†ç»„
        batch_size = min(unitary_dim, len(connected_nodes))

        for i in range(0, len(connected_nodes), batch_size):
            end_idx = min(i + batch_size, len(connected_nodes))
            batch_nodes = connected_nodes[i:end_idx]
            actual_batch_size = len(batch_nodes)

            # æ„å»ºæ‰¹æ¬¡ç‰¹å¾çŸ©é˜µ
            batch_features = x_complex[batch_nodes]  # [actual_batch_size, output_dim]
            # åˆ›å»ºé€‚é…çš„é…‰çŸ©é˜µ
            if actual_batch_size < unitary_dim:
                # ä½¿ç”¨é…‰çŸ©é˜µçš„å­å—
                U_sub = U_dilation[:actual_batch_size, :actual_batch_size]
            else:
                U_sub = U_dilation

            try:
                # æ‰¹é‡æ¼”åŒ–ï¼š[actual_batch_size, output_dim]
                evolved_batch = U_sub @ batch_features
                evolved_x[batch_nodes] = evolved_batch

            except Exception as e:
                # å¤±è´¥æ—¶ä¿æŒåŸç‰¹å¾
                continue
            # print(evolved_x.size())

        return evolved_x

    def message(self, x_j):
        """æ¶ˆæ¯å‡½æ•°"""
        # å¤„ç†å¤æ•°è¾“å…¥
        if x_j.dtype == torch.cfloat:
            # å¯¹å¤æ•°ç‰¹å¾ï¼Œåˆ†åˆ«å¤„ç†å®éƒ¨å’Œè™šéƒ¨
            weights = self.message_weights.to(x_j.device)
            return x_j @ weights
        else:
            weights = self.message_weights.real.to(x_j.device)
            return x_j @ weights

    def update(self, aggr_out):
        """æ›´æ–°å‡½æ•°"""
        return complex_dropout(aggr_out, p=self.dropout, training=self.training)

    def message(self, x_j):
        """æ¶ˆæ¯å‡½æ•°"""
        # å¤„ç†å¤æ•°è¾“å…¥
        if x_j.dtype == torch.cfloat:
            # å¯¹å¤æ•°ç‰¹å¾ï¼Œåˆ†åˆ«å¤„ç†å®éƒ¨å’Œè™šéƒ¨
            weights = self.message_weights.to(x_j.device)
            return x_j @ weights
        else:
            weights = self.message_weights.real.to(x_j.device)
            return x_j @ weights

    def update(self, aggr_out):
        """æ›´æ–°å‡½æ•°"""
        return complex_dropout(aggr_out, p=self.dropout, training=self.training)


class QuantumGraphConvNet(nn.Module):
    """ä¿®å¤+å¢å¼ºç‰ˆï¼šå‰ç½®GCN+Poolå‹ç¼©ï¼Œåæ¥å¤æ•°é‡å­å±‚"""

    def __init__(self, input_dim, hidden_dims, num_classes, dropout=0.1, num_layers=2):
        """
        hidden_dims ç¤ºä¾‹: [gcn_hidden1, gcn_hidden2, quantum_hidden1, quantum_hidden2, ..., quantum_hiddenN]
        """
        super().__init__()
        assert len(hidden_dims) >= 3, "hidden_dims è‡³å°‘åŒ…å« GCN ä¸¤å±‚ + 1 å±‚é‡å­"

        self.input_dim = input_dim
        self.hidden_dims = hidden_dims
        self.num_classes = num_classes
        self.dropout = dropout
        self.num_layers = num_layers

        # GCN + Pooling
        self.gcn1 = GCNConv(input_dim, hidden_dims[0])
        self.gcn2 = GCNConv(hidden_dims[0], hidden_dims[1])
        self.pool1 = TopKPooling(hidden_dims[0], ratio=0.8)
        self.pool2 = TopKPooling(hidden_dims[1], ratio=0.2)
        # åˆ†ç¦»å®éƒ¨è™šéƒ¨æ˜ å°„ï¼ˆè¾“å…¥ä¸º pooling åçš„ gcn2 è¾“å‡ºï¼‰
        self.input_real_proj = nn.Linear(hidden_dims[1], hidden_dims[2])
        self.input_imag_proj = nn.Linear(hidden_dims[1], hidden_dims[2])

        # æ„å»º LocalQuantumLayer å †å 
        self.quantum_layers = nn.ModuleList()
        quantum_dims = hidden_dims[2:]
        layer_dims = [hidden_dims[2]] + quantum_dims  # ç”¨äºé‡å­å±‚é“¾
        # print(num_layers)
        # print(layer_dims)
        for i in range(num_layers-2):
            layer = LocalQuantumLayer(
                input_dim=layer_dims[i],
                output_dim=layer_dims[i + 1] if i + 1 < len(layer_dims) else layer_dims[i],
                dropout=dropout
            )
            self.quantum_layers.append(layer)

        final_dim = layer_dims[-1]
        self.magnitude_classifier = nn.Sequential(
            nn.Linear(final_dim, final_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(final_dim // 2, num_classes)
        )
        self.phase_classifier = nn.Sequential(
            nn.Linear(final_dim, final_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(final_dim // 2, num_classes)
        )

        self.global_pool = global_mean_pool

    def forward(self, data):
        x, edge_index, batch = data.x, data.edge_index, data.batch

        # ----- GCN + Pooling -----
        x = F.relu(self.gcn1(x, edge_index))
        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, batch=batch)
        x = F.relu(self.gcn2(x, edge_index))
        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, batch=batch)

        # ----- åˆå§‹åŒ–å¤æ•°è¾“å…¥ -----
        x_real = self.input_real_proj(x)
        x_imag = self.input_imag_proj(x)

        # ----- å¤šå±‚ LocalQuantumLayer æ¼”åŒ– -----
        for layer in self.quantum_layers:
            x_real_new, x_imag_new = layer(x_real, x_imag, edge_index)

            # å¯é€‰ï¼šæ®‹å·®è¿æ¥
            # x_real_new = x_real_new + x_real
            # x_imag_new = x_imag_new + x_imag

            # å±‚å½’ä¸€åŒ–
            x_real = F.layer_norm(x_real_new, x_real_new.shape[-1:])
            x_imag = F.layer_norm(x_imag_new, x_imag_new.shape[-1:])

        # ----- å¤æ•°æ¨¡é•¿å’Œç›¸ä½ -----
        x_complex = torch.complex(x_real, x_imag)
        magnitude = torch.abs(x_complex)
        phase = torch.angle(x_complex)

        # ----- å›¾çº§æ± åŒ– -----
        if batch is not None:
            magnitude_pooled = self.global_pool(magnitude, batch)
            phase_pooled = self.global_pool(phase, batch)
        else:
            magnitude_pooled = magnitude.mean(dim=0, keepdim=True)
            phase_pooled = phase.mean(dim=0, keepdim=True)

        # ----- åˆ†ç±» -----
        magnitude_logits = self.magnitude_classifier(magnitude_pooled)
        phase_logits = self.phase_classifier(phase_pooled)

        final_logits = magnitude_logits + 0.1 * phase_logits

        # return final_logits, magnitude_pooled, phase_pooled
        return F.log_softmax(final_logits, dim=1)

def benchmark_quantum_model(name, num_nodes, num_features, model_dims, num_classes=10, num_runs=10):
    print(f"\nğŸ”¥ {name}")
    # print(f"ğŸš€ æ„å»ºæ•°å€¼ç¨³å®šçš„é—ªç”µçº§é‡å­GCN: {model_dims}")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    edge_index = safe_erdos_renyi_graph(num_nodes, edge_prob=0.1, device=device)
    x = torch.randn(num_nodes, num_features, device=device) * 0.01
    batch = torch.zeros(num_nodes, dtype=torch.long, device=device)
    data = Data(x=x, edge_index=edge_index, batch=batch)

    model = QuantumGraphConvNet(
    input_dim=num_features,
    hidden_dims = model_dims,
    num_classes=num_classes,
    dropout=0.1,
    num_layers=3
).to(device)

    total_params = sum(p.numel() for p in model.parameters())
    print(f"æ¨¡å‹å‚æ•°: {total_params:,}")
    # print(model)
    model.eval()
    torch.cuda.reset_peak_memory_stats()

    times = []
    num_abnormal = 0
    output_sum = 0.0

    with torch.no_grad():
        for _ in range(num_runs):
            start = time.time()
            output = model(data)
            torch.cuda.synchronize()
            end = time.time()

            # âœ… æå– logits
            logits = output[0] if isinstance(output, tuple) else output

            # âœ… æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
            if torch.isnan(logits).any() or torch.isinf(logits).any():
                num_abnormal += 1

            # âœ… softmax æ€»å’Œæ£€æŸ¥
            output_sum += torch.exp(logits).sum(dim=1).mean().item()

            times.append((end - start) * 1000)  # ms

    avg_time = sum(times) / num_runs
    throughput = 1000.0 / avg_time
    mean_prob_sum = output_sum / num_runs
    max_mem = torch.cuda.max_memory_allocated(device) / (1024 ** 2)

    print(f"  âš¡ å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.2f} ms")
    print(f"  ğŸ“Š ååé‡: {throughput:.1f} graphs/sec")
    print(f"  âš ï¸ æ•°å€¼å¼‚å¸¸æ¬¡æ•°: {num_abnormal}/{num_runs}")
    print(f"  âœ… è¾“å‡ºç¨³å®šæ€§: {'å®Œå…¨ç¨³å®š' if num_abnormal == 0 else 'ä¸ç¨³å®š'}")
    print(f"  ğŸ¯ è¾“å‡ºæ¦‚ç‡å’Œ: {mean_prob_sum:.4f}")
    print(f"  ğŸ’¾ å³°å€¼GPUå†…å­˜: {max_mem:.1f} MB")

def safe_erdos_renyi_graph(num_nodes, edge_prob=0.1, device=None):
    edge_index = erdos_renyi_graph(num_nodes, edge_prob=edge_prob).to(device)
    # å¦‚æœè¾¹ä¸ºç©ºï¼Œåˆ™æ‰‹åŠ¨åŠ ä¸€æ¡è‡ªç¯è¾¹
    if edge_index.numel() == 0:
        edge_index = torch.tensor([[0], [0]], device=device)
    return edge_index
def benchmark_batchsize_sweep(title, num_graphs, num_nodes, num_features, model_dims,
                               batch_sizes=[1, 2, 4, 8, 16, 32], num_classes=10, num_batches=10):
    print(f"\nğŸš€ ã€{title}ã€‘Batch Size æ€§èƒ½æ‰«æ")
    print("=" * 60)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # æ„é€ å›¾æ•°æ®é›†
    dataset = []
    for _ in range(num_graphs):
        edge_index = safe_erdos_renyi_graph(num_nodes, edge_prob=0.15)
        x = torch.randn(num_nodes, num_features)
        dataset.append(Data(x=x, edge_index=edge_index))

    for batch_size in batch_sizes:
        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)

        model = QuantumGraphConvNet(
            input_dim=num_features,
            hidden_dims=model_dims,
            num_classes=num_classes,
            dropout=0.1,
            num_layers=3
        ).to(device)

        model.eval()
        torch.cuda.reset_peak_memory_stats()

        times = []
        num_abnormal = 0
        output_sum = 0.0
        total_graphs = 0

        with torch.no_grad():
            for i, batch in enumerate(loader):
                if i >= num_batches:
                    break

                batch = batch.to(device)
                start = time.time()
                output = model(batch)
                torch.cuda.synchronize()
                end = time.time()

                logits = output[0] if isinstance(output, tuple) else output
                if torch.isnan(logits).any() or torch.isinf(logits).any():
                    num_abnormal += 1

                output_sum += torch.exp(logits).sum(dim=1).mean().item()
                times.append((end - start) * 1000)
                total_graphs += logits.shape[0]

        avg_time = sum(times) / num_batches
        throughput = (total_graphs * 1000.0) / sum(times)
        mean_prob_sum = output_sum / num_batches
        max_mem = torch.cuda.max_memory_allocated(device) / (1024 ** 2)

        print(f"[BatchSize={batch_size:>2}] âš¡ {avg_time:.2f} ms/batch | ğŸ“Š {throughput:.1f} graphs/sec | ğŸ’¾ {max_mem:.1f} MB | ğŸ¯ Psum={mean_prob_sum:.3f} | âœ… {'âœ”ï¸' if num_abnormal==0 else 'âŒ'}")

def run_all_quantum_benchmarks():
    print("ğŸš€ å¯åŠ¨ QuantumGCN å¤šåœºæ™¯åŸºå‡†æµ‹è¯•")
    print("=" * 60)

    benchmark_quantum_model(
        name="å°å›¾: 200 èŠ‚ç‚¹, 32 ç‰¹å¾",
        num_nodes=200,
        num_features=32,
        model_dims=[32, 16, 16]
    )

    benchmark_quantum_model(
        name="ä¸­ç­‰å›¾: 500 èŠ‚ç‚¹, 64 ç‰¹å¾",
        num_nodes=500,
        num_features=64,
        model_dims=[32, 16, 16]
    )

    benchmark_quantum_model(
        name="å¤§å›¾: 1000 èŠ‚ç‚¹, 128 ç‰¹å¾",
        num_nodes=1000,
        num_features=128,
        model_dims=[32, 16, 16]
    )

    benchmark_quantum_model(
        name="ç‰¹å¾å¤§å›¾: 10 èŠ‚ç‚¹, 1323 ç‰¹å¾",
        num_nodes=10,
        num_features=1323,
        model_dims=[32, 16, 16]
    )

    print("\nâš¡ æ•°å€¼ç¨³å®šæ€§ä¼˜åŒ–æ€»ç»“:")
    print("  ğŸ”§ ç›¸ä½è§’åº¦é™åˆ¶ - é˜²æ­¢ä¸‰è§’å‡½æ•°ä¸ç¨³å®š")
    print("  ğŸ”§ æ¢¯åº¦è£å‰ª - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸")
    print("  ğŸ”§ å‚æ•°èŒƒå›´çº¦æŸ - ç¡®ä¿æ•°å€¼ç¨³å®š")
    print("  ğŸ”§ å±‚å½’ä¸€åŒ– - ç¨³å®šä¸­é—´æ¿€æ´»")
    print("  ğŸ”§ ä¿å®ˆåˆå§‹åŒ– - é™ä½å‘æ•£é£é™©")
    print("  ğŸ”§ å¼‚å¸¸å€¼æ£€æµ‹ - è¿è¡Œæ—¶NaN/Infå¤„ç†")
    print("  âœ… å®Œæ•´å¤æ•°æ¼”åŒ– - ä¿æŒ")
    print("  âœ… æ¨¡é•¿åˆ†ç±» - ä¿æŒ")
    print("  âœ… æ®‹å·®è¿æ¥ - ä¿æŒ")
    print("  âœ… å±€éƒ¨é…‰æ€§ - ä¿æŒ")
    print("  âœ… é…‰æ‹“å±•æ€§ U=[G/Î», I-GGâ€ /Î»; I-GGâ€ /Î», -Gâ€ /Î»] - å®Œæ•´å®ç°")
    print("  ğŸ¯ ç›®æ ‡: å½»åº•æ¶ˆé™¤NaNé—®é¢˜ï¼Œä¿æŒé‡å­ç‰¹æ€§ï¼")

if __name__ == "__main__":
    run_all_quantum_benchmarks()

    benchmark_batchsize_sweep(
        title="å°å›¾æµ‹è¯•: 200 èŠ‚ç‚¹ Ã— 32 ç‰¹å¾",
        num_graphs=128,
        num_nodes=200,
        num_features=32,
        model_dims=[32, 16, 16],
        batch_sizes=[1,2,4,8,16,32,64]
    )