# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# from torch_geometric.nn import MessagePassing
# from torch_geometric.nn import global_mean_pool
# from complexPyTorch.complexLayers import ComplexLinear
# from complexPyTorch.complexFunctions import complex_relu, complex_dropout
# from torch_sparse import SparseTensor
# import math
#
#
# class OptimizedComplexUnitaryEvolutionGCNConv(MessagePassing):
#     """GPUä¼˜åŒ–çš„å¤æ•°é…‰æ¼”åŒ–GCNå·ç§¯å±‚"""
#
#     def __init__(self, in_channels, out_channels,
#                  evolution_time=1.0,
#                  normalize=True,
#                  bias=True,
#                  dropout=0.0,
#                  activation='complex_relu',
#                  max_matrix_exp_terms=10):
#         super(OptimizedComplexUnitaryEvolutionGCNConv, self).__init__(aggr='add')
#
#         self.in_channels = in_channels
#         self.out_channels = out_channels
#         self.evolution_time = evolution_time
#         self.normalize = normalize
#         self.dropout = dropout
#         self.activation = activation
#         self.max_matrix_exp_terms = max_matrix_exp_terms
#
#         # å¤æ•°ç‰¹å¾å˜æ¢å±‚
#         self.complex_lin = ComplexLinear(in_channels, out_channels)
#
#         # å¯å­¦ä¹ çš„æ¼”åŒ–æ—¶é—´å‚æ•°
#         self.time_param_real = nn.Parameter(torch.tensor(evolution_time, dtype=torch.float32))
#         self.time_param_imag = nn.Parameter(torch.zeros(1, dtype=torch.float32))
#
#         # é¢å¤–çš„å¤æ•°åç½®
#         self.use_extra_bias = bias
#         if self.use_extra_bias:
#             self.extra_bias_real = nn.Parameter(torch.Tensor(out_channels))
#             self.extra_bias_imag = nn.Parameter(torch.Tensor(out_channels))
#         else:
#             self.register_parameter('extra_bias_real', None)
#             self.register_parameter('extra_bias_imag', None)
#
#         self.reset_parameters()
#
#     def reset_parameters(self):
#         if self.extra_bias_real is not None:
#             nn.init.zeros_(self.extra_bias_real)
#             nn.init.zeros_(self.extra_bias_imag)
#
#     @property
#     def complex_time_param(self):
#         """è·å–å¤æ•°æ—¶é—´å‚æ•°"""
#         return torch.complex(self.time_param_real, self.time_param_imag)
#
#     @property
#     def complex_extra_bias(self):
#         """è·å–é¢å¤–çš„å¤æ•°åç½®"""
#         if self.extra_bias_real is not None:
#             return torch.complex(self.extra_bias_real, self.extra_bias_imag)
#         return None
#
#     def matrix_exp_taylor(self, A, max_terms=10):
#         """ä½¿ç”¨æ³°å‹’çº§æ•°åœ¨GPUä¸Šè®¡ç®—çŸ©é˜µæŒ‡æ•°"""
#         device = A.device
#         dtype = A.dtype
#
#         # åˆå§‹åŒ–ç»“æœçŸ©é˜µ
#         result = torch.eye(A.shape[-1], device=device, dtype=dtype).expand_as(A)
#         term = torch.eye(A.shape[-1], device=device, dtype=dtype).expand_as(A)
#
#         # æ³°å‹’çº§æ•°å±•å¼€
#         for i in range(1, max_terms + 1):
#             term = torch.matmul(term, A) / i
#             result = result + term
#
#         return result
#
#     def build_adjacency_matrices_batch(self, edge_index, num_nodes, x):
#         """æ‰¹é‡æ„å»ºæ‰€æœ‰èŠ‚ç‚¹çš„é‚»æ¥çŸ©é˜µ"""
#         device = x.device
#         dtype = torch.complex64
#
#         # åˆ›å»ºå®Œæ•´çš„é‚»æ¥çŸ©é˜µ
#         adj_matrix = torch.zeros(num_nodes, num_nodes, device=device, dtype=dtype)
#
#         # å¡«å……é‚»æ¥çŸ©é˜µ
#         adj_matrix[edge_index[0], edge_index[1]] = 1.0 + 0.0j
#
#         # ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ›å»ºåŒ…å«è‡ªèº«å’Œé‚»å±…çš„å­å›¾é‚»æ¥çŸ©é˜µ
#         # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç®€åŒ–çš„æ–¹æ³•ï¼šç›´æ¥ä½¿ç”¨åº¦æ•°ä½œä¸ºå­å›¾å¤§å°
#         degrees = torch.zeros(num_nodes, device=device, dtype=torch.long)
#         degrees = degrees.scatter_add(0, edge_index[0], torch.ones_like(edge_index[0]))
#         degrees = degrees.scatter_add(0, edge_index[1], torch.ones_like(edge_index[1]))
#
#         # é™åˆ¶æœ€å¤§é‚»å±…æ•°ä»¥æ§åˆ¶è®¡ç®—å¤æ‚åº¦
#         max_neighbors = min(10, degrees.max().item())
#
#         # åˆ›å»ºæ‰¹é‡é‚»æ¥çŸ©é˜µ (num_nodes, max_size, max_size)
#         max_size = max_neighbors + 1  # +1 for the node itself
#         batch_adj = torch.zeros(num_nodes, max_size, max_size, device=device, dtype=dtype)
#
#         # ä¸ºæ¯ä¸ªèŠ‚ç‚¹æ„å»ºå­å›¾
#         for node_idx in range(num_nodes):
#             # æ‰¾åˆ°é‚»å±…
#             neighbors = []
#             mask = (edge_index[0] == node_idx) | (edge_index[1] == node_idx)
#             if mask.any():
#                 node_edges = edge_index[:, mask]
#                 for src, dst in node_edges.t():
#                     if src == node_idx:
#                         neighbors.append(dst.item())
#                     else:
#                         neighbors.append(src.item())
#
#             # å»é‡å¹¶é™åˆ¶æ•°é‡
#             neighbors = list(set(neighbors))[:max_neighbors - 1]
#             subgraph_nodes = [node_idx] + neighbors
#
#             # æ„å»ºå­å›¾é‚»æ¥çŸ©é˜µ
#             for i, node_i in enumerate(subgraph_nodes):
#                 for j, node_j in enumerate(subgraph_nodes):
#                     if adj_matrix[node_i, node_j] != 0:
#                         batch_adj[node_idx, i, j] = 1.0 + 0.0j
#
#         return batch_adj
#
#     def batch_unitary_evolution(self, batch_adj, complex_time):
#         """æ‰¹é‡è®¡ç®—é…‰æ¼”åŒ–çŸ©é˜µ"""
#         device = batch_adj.device
#         num_nodes, max_size, _ = batch_adj.shape
#
#         # è®¡ç®— -i * A * t
#         evolution_arg = -1j * batch_adj * complex_time
#
#         # ä½¿ç”¨æ³°å‹’çº§æ•°è®¡ç®—çŸ©é˜µæŒ‡æ•°
#         G_t = self.matrix_exp_taylor(evolution_arg, self.max_matrix_exp_terms)
#
#         # è®¡ç®—é…‰æ¼”åŒ–çŸ©é˜µçš„ç®€åŒ–ç‰ˆæœ¬
#         # è¿™é‡Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨G_tçš„å½’ä¸€åŒ–ç‰ˆæœ¬
#         # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è¿›è¡Œæ›´å¤æ‚çš„é…‰åŒ–å¤„ç†
#         norms = torch.norm(G_t, dim=(-2, -1), keepdim=True)
#         G_t_normalized = G_t / (norms + 1e-8)
#
#         return G_t_normalized
#
#     def batch_evolve_features(self, x, batch_adj, complex_time):
#         """æ‰¹é‡æ¼”åŒ–èŠ‚ç‚¹ç‰¹å¾"""
#         device = x.device
#         num_nodes, feature_dim = x.shape
#         max_size = batch_adj.shape[1]
#
#         # è®¡ç®—æ¼”åŒ–çŸ©é˜µ
#         U = self.batch_unitary_evolution(batch_adj, complex_time)
#
#         # å‡†å¤‡ç‰¹å¾çŸ©é˜µ
#         # ä¸ºæ¯ä¸ªèŠ‚ç‚¹å‡†å¤‡å…¶å­å›¾ç‰¹å¾
#         batch_features = torch.zeros(num_nodes, max_size, feature_dim,
#                                      device=device, dtype=x.dtype)
#
#         # ç®€åŒ–ç‰ˆæœ¬ï¼šæ¯ä¸ªèŠ‚ç‚¹çš„ç¬¬ä¸€ä¸ªä½ç½®æ˜¯è‡ªå·±ï¼Œå…¶ä»–ä½ç½®å¡«å……é‚»å±…ç‰¹å¾çš„å¹³å‡å€¼
#         batch_features[:, 0, :] = x
#
#         # å¯¹äºå…¶ä»–ä½ç½®ï¼Œæˆ‘ä»¬ä½¿ç”¨é‚»åŸŸç‰¹å¾çš„èšåˆ
#         for node_idx in range(num_nodes):
#             # æ‰¾åˆ°é‚»å±…å¹¶èšåˆç‰¹å¾
#             neighbors = []
#             edge_mask = (batch_adj[node_idx, 0, 1:] != 0)
#             if edge_mask.any():
#                 # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œä½¿ç”¨å¹³å‡æ± åŒ–
#                 neighbor_features = x.mean(dim=0, keepdim=True)
#                 for i in range(1, min(max_size, edge_mask.sum().item() + 1)):
#                     batch_features[node_idx, i, :] = neighbor_features
#
#         # åº”ç”¨æ¼”åŒ–çŸ©é˜µ
#         # U: (num_nodes, max_size, max_size)
#         # batch_features: (num_nodes, max_size, feature_dim)
#         evolved_features = torch.matmul(U, batch_features)
#
#         # è¿”å›æ¯ä¸ªèŠ‚ç‚¹æ¼”åŒ–åçš„ç‰¹å¾ï¼ˆå–ç¬¬ä¸€ä¸ªä½ç½®ï¼‰
#         return evolved_features[:, 0, :]
#
#     def complex_activation(self, x):
#         """åº”ç”¨å¤æ•°æ¿€æ´»å‡½æ•°"""
#         if self.activation == 'complex_relu':
#             return complex_relu(x)
#         elif self.activation == 'none':
#             return x
#         else:
#             return complex_relu(x)
#
#     def prepare_complex_input(self, x):
#         """å°†å®æ•°è¾“å…¥è½¬æ¢ä¸ºå¤æ•°å½¢å¼"""
#         if x.dtype in [torch.complex64, torch.complex128]:
#             return x
#         else:
#             return torch.complex(x, torch.zeros_like(x))
#
#     def forward(self, x, edge_index):
#         num_nodes = x.size(0)
#
#         # ç¡®ä¿è¾“å…¥æ˜¯å¤æ•°å½¢å¼
#         x_complex = self.prepare_complex_input(x)
#
#         # å¤æ•°ç‰¹å¾å˜æ¢
#         x_transformed = self.complex_lin(x_complex)
#
#         # åº”ç”¨å¤æ•°æ¿€æ´»å‡½æ•°
#         x_transformed = self.complex_activation(x_transformed)
#
#         # åº”ç”¨å¤æ•°dropout
#         if self.training and self.dropout > 0:
#             x_transformed = complex_dropout(x_transformed, p=self.dropout, training=self.training)
#
#         # æ‰¹é‡æ„å»ºé‚»æ¥çŸ©é˜µ
#         batch_adj = self.build_adjacency_matrices_batch(edge_index, num_nodes, x_transformed)
#
#         # æ‰¹é‡æ¼”åŒ–ç‰¹å¾
#         evolved_features = self.batch_evolve_features(x_transformed, batch_adj, self.complex_time_param)
#
#         # æ·»åŠ é¢å¤–çš„å¤æ•°åç½®
#         if self.complex_extra_bias is not None:
#             evolved_features = evolved_features + self.complex_extra_bias
#
#         return evolved_features
#
#
# class OptimizedComplexUnitaryGCN(nn.Module):
#     """ä¼˜åŒ–çš„å¤šå±‚å¤æ•°é…‰æ¼”åŒ–GCNç½‘ç»œ"""
#
#     def __init__(self, input_dim, hidden_dims, output_dim,
#                  evolution_times=None, dropout=0.0,
#                  max_matrix_exp_terms=8):
#         super(OptimizedComplexUnitaryGCN, self).__init__()
#
#         # æ„å»ºå±‚åˆ—è¡¨
#         dims = [input_dim] + hidden_dims + [output_dim]
#         self.layers = nn.ModuleList()
#
#         if evolution_times is None:
#             evolution_times = [1.0] * (len(dims) - 1)
#
#         for i in range(len(dims) - 1):
#             layer = OptimizedComplexUnitaryEvolutionGCNConv(
#                 in_channels=dims[i],
#                 out_channels=dims[i + 1],
#                 evolution_time=evolution_times[i],
#                 dropout=dropout if i < len(dims) - 2 else 0.0,
#                 max_matrix_exp_terms=max_matrix_exp_terms
#             )
#             self.layers.append(layer)
#
#     def forward(self, data):
#         x, edge_index, batch = data.x, data.edge_index, data.batch
#
#         # å‰å‘ä¼ æ’­
#         for layer in self.layers:
#             x = layer(x, edge_index)
#
#         # å–å¹…åº¦å¹¶è¿›è¡Œå›¾æ± åŒ–
#         x = x.abs()
#         x = global_mean_pool(x, batch)
#         x = F.log_softmax(x, dim=1)
#
#         return x
#
#
# # è¿›ä¸€æ­¥ä¼˜åŒ–çš„ç‰ˆæœ¬ - ä½¿ç”¨æ¶ˆæ¯ä¼ é€’æœºåˆ¶
# class MessagePassingComplexUnitaryGCN(MessagePassing):
#     """åŸºäºæ¶ˆæ¯ä¼ é€’çš„å¤æ•°é…‰æ¼”åŒ–GCN - æ›´é«˜æ•ˆçš„GPUå®ç°"""
#
#     def __init__(self, in_channels, out_channels, evolution_time=1.0,
#                  dropout=0.0, activation='complex_relu'):
#         super(MessagePassingComplexUnitaryGCN, self).__init__(aggr='add')
#
#         self.in_channels = in_channels
#         self.out_channels = out_channels
#         self.dropout = dropout
#         self.activation = activation
#
#         # å¤æ•°çº¿æ€§å˜æ¢
#         self.complex_lin = ComplexLinear(in_channels, out_channels)
#
#         # æ¼”åŒ–å‚æ•°
#         self.time_param_real = nn.Parameter(torch.tensor(evolution_time, dtype=torch.float32))
#         self.time_param_imag = nn.Parameter(torch.zeros(1, dtype=torch.float32))
#
#         # æ¶ˆæ¯ä¼ é€’çš„æƒé‡
#         self.msg_lin = ComplexLinear(out_channels, out_channels)
#
#     @property
#     def complex_time_param(self):
#         return torch.complex(self.time_param_real, self.time_param_imag)
#
#     def message(self, x_j, x_i):
#         """è®¡ç®—æ¶ˆæ¯ - æ¨¡æ‹Ÿé‡å­æ¼”åŒ–çš„æ¶ˆæ¯ä¼ é€’"""
#         # åº”ç”¨å¤æ•°æ—¶é—´æ¼”åŒ–
#         time_factor = torch.exp(-1j * self.complex_time_param)
#         evolved_msg = self.msg_lin(x_j) * time_factor
#         return evolved_msg
#
#     def update(self, aggr_out, x):
#         """æ›´æ–°èŠ‚ç‚¹ç‰¹å¾"""
#         # ç»“åˆåŸå§‹ç‰¹å¾å’Œèšåˆæ¶ˆæ¯
#         evolution_factor = torch.exp(-1j * self.complex_time_param * 0.5)
#         updated = x * evolution_factor + aggr_out * (1 - evolution_factor)
#         return updated
#
#     def forward(self, x, edge_index):
#         # ç¡®ä¿è¾“å…¥æ˜¯å¤æ•°å½¢å¼
#         if x.dtype not in [torch.complex64, torch.complex128]:
#             x = torch.complex(x, torch.zeros_like(x))
#
#         # çº¿æ€§å˜æ¢
#         x = self.complex_lin(x)
#
#         # åº”ç”¨æ¿€æ´»å‡½æ•°
#         if self.activation == 'complex_relu':
#             x = complex_relu(x)
#
#         # åº”ç”¨dropout
#         if self.training and self.dropout > 0:
#             x = complex_dropout(x, p=self.dropout, training=self.training)
#
#         # æ¶ˆæ¯ä¼ é€’
#         x = self.propagate(edge_index, x=x)
#
#         return x
#
#
# # ä½¿ç”¨ç¤ºä¾‹å’Œæ€§èƒ½æµ‹è¯•
# def test_optimized_models():
#     """æµ‹è¯•ä¼˜åŒ–åçš„æ¨¡å‹æ€§èƒ½"""
#     import time
#
#     print("ğŸš€ GPUä¼˜åŒ–çš„å¤æ•°é…‰æ¼”åŒ–GCNæµ‹è¯•")
#     print("=" * 50)
#
#     # åˆ›å»ºæµ‹è¯•æ•°æ®
#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#     print(f"ä½¿ç”¨è®¾å¤‡: {device}")
#
#     num_nodes = 1000
#     num_features = 64
#     num_classes = 10
#
#     # ç”Ÿæˆéšæœºå›¾æ•°æ®
#     x = torch.randn(num_nodes, num_features, device=device)
#
#     # åˆ›å»ºéšæœºè¾¹
#     num_edges = num_nodes * 5
#     edge_index = torch.randint(0, num_nodes, (2, num_edges), device=device)
#
#     # åˆ›å»ºbatchï¼ˆç”¨äºå›¾çº§åˆ«ä»»åŠ¡ï¼‰
#     batch = torch.zeros(num_nodes, dtype=torch.long, device=device)
#
#     class TestData:
#         def __init__(self, x, edge_index, batch):
#             self.x = x
#             self.edge_index = edge_index
#             self.batch = batch
#
#     data = TestData(x, edge_index, batch)
#
#     # æµ‹è¯•åŸå§‹æ¨¡å‹
#     print("ğŸ“Š åŸå§‹æ¨¡å‹æ€§èƒ½æµ‹è¯•ï¼ˆCPUå¯†é›†å‹ï¼‰...")
#     # è¿™é‡Œæˆ‘ä»¬ä¸è¿è¡ŒåŸå§‹æ¨¡å‹ï¼Œå› ä¸ºå®ƒä¼šå¾ˆæ…¢
#
#     # æµ‹è¯•ä¼˜åŒ–æ¨¡å‹1
#     print("ğŸ“Š ä¼˜åŒ–æ¨¡å‹1æ€§èƒ½æµ‹è¯•ï¼ˆæ‰¹é‡çŸ©é˜µè¿ç®—ï¼‰...")
#     model1 = OptimizedComplexUnitaryGCN(
#         input_dim=num_features,
#         hidden_dims=[32, 16],
#         output_dim=num_classes,
#         dropout=0.1,
#         max_matrix_exp_terms=6
#     ).to(device)
#
#     # é¢„çƒ­
#     with torch.no_grad():
#         _ = model1(data)
#
#     # è®¡æ—¶
#     start_time = time.time()
#     for _ in range(10):
#         with torch.no_grad():
#             output1 = model1(data)
#     end_time = time.time()
#
#     print(f"   å¹³å‡æ¨ç†æ—¶é—´: {(end_time - start_time) / 10:.4f}s")
#     print(f"   è¾“å‡ºå½¢çŠ¶: {output1.shape}")
#     print(f"   GPUå†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated(device) / 1024 ** 2:.2f}MB")
#
#     # æµ‹è¯•ä¼˜åŒ–æ¨¡å‹2
#     print("ğŸ“Š ä¼˜åŒ–æ¨¡å‹2æ€§èƒ½æµ‹è¯•ï¼ˆæ¶ˆæ¯ä¼ é€’ï¼‰...")
#     model2 = MessagePassingComplexUnitaryGCN(
#         in_channels=num_features,
#         out_channels=num_classes,
#         evolution_time=1.0,
#         dropout=0.1
#     ).to(device)
#
#     # é¢„çƒ­
#     with torch.no_grad():
#         _ = model2(x, edge_index)
#
#     # è®¡æ—¶
#     start_time = time.time()
#     for _ in range(10):
#         with torch.no_grad():
#             output2 = model2(x, edge_index)
#     end_time = time.time()
#
#     print(f"   å¹³å‡æ¨ç†æ—¶é—´: {(end_time - start_time) / 10:.4f}s")
#     print(f"   è¾“å‡ºå½¢çŠ¶: {output2.shape}")
#     print(f"   GPUå†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated(device) / 1024 ** 2:.2f}MB")
#
#     # æµ‹è¯•æ¢¯åº¦è®¡ç®—
#     print("ğŸ“Š æ¢¯åº¦è®¡ç®—æµ‹è¯•...")
#     model2.train()
#     optimizer = torch.optim.Adam(model2.parameters(), lr=0.01)
#
#     output = model2(x, edge_index)
#     y_true = torch.randint(0, num_classes, (num_nodes,), device=device)
#     loss = F.cross_entropy(output.real, y_true)
#
#     start_time = time.time()
#     loss.backward()
#     optimizer.step()
#     end_time = time.time()
#
#     print(f"   åå‘ä¼ æ’­æ—¶é—´: {end_time - start_time:.4f}s")
#     print(f"   æŸå¤±å€¼: {loss.item():.4f}")
#
#     print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
#
#
# if __name__ == "__main__":
#     test_optimized_models()


import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import MessagePassing
from torch_geometric.nn import global_mean_pool
from complexPyTorch.complexLayers import ComplexLinear
from complexPyTorch.complexFunctions import complex_relu, complex_dropout
from torch_sparse import SparseTensor
import math


class OptimizedComplexUnitaryEvolutionGCNConv(MessagePassing):
    """GPUä¼˜åŒ–çš„å¤æ•°é…‰æ¼”åŒ–GCNå·ç§¯å±‚"""

    def __init__(self, in_channels, out_channels,
                 evolution_time=1.0,
                 normalize=True,
                 bias=True,
                 dropout=0.0,
                 activation='complex_relu',
                 max_matrix_exp_terms=10):
        super(OptimizedComplexUnitaryEvolutionGCNConv, self).__init__(aggr='add')

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.evolution_time = evolution_time
        self.normalize = normalize
        self.dropout = dropout
        self.activation = activation
        self.max_matrix_exp_terms = max_matrix_exp_terms

        # å¤æ•°ç‰¹å¾å˜æ¢å±‚
        self.complex_lin = ComplexLinear(in_channels, out_channels)

        # å¯å­¦ä¹ çš„æ¼”åŒ–æ—¶é—´å‚æ•°
        self.time_param_real = nn.Parameter(torch.tensor(evolution_time, dtype=torch.float32))
        self.time_param_imag = nn.Parameter(torch.zeros(1, dtype=torch.float32))

        # é¢å¤–çš„å¤æ•°åç½®
        self.use_extra_bias = bias
        if self.use_extra_bias:
            self.extra_bias_real = nn.Parameter(torch.Tensor(out_channels))
            self.extra_bias_imag = nn.Parameter(torch.Tensor(out_channels))
        else:
            self.register_parameter('extra_bias_real', None)
            self.register_parameter('extra_bias_imag', None)

        self.reset_parameters()

    def reset_parameters(self):
        if self.extra_bias_real is not None:
            nn.init.zeros_(self.extra_bias_real)
            nn.init.zeros_(self.extra_bias_imag)

    @property
    def complex_time_param(self):
        """è·å–å¤æ•°æ—¶é—´å‚æ•°"""
        return torch.complex(self.time_param_real, self.time_param_imag)

    @property
    def complex_extra_bias(self):
        """è·å–é¢å¤–çš„å¤æ•°åç½®"""
        if self.extra_bias_real is not None:
            return torch.complex(self.extra_bias_real, self.extra_bias_imag)
        return None

    def matrix_exp_taylor(self, A, max_terms=10):
        """ä½¿ç”¨æ³°å‹’çº§æ•°åœ¨GPUä¸Šè®¡ç®—çŸ©é˜µæŒ‡æ•°"""
        device = A.device
        dtype = A.dtype

        # åˆå§‹åŒ–ç»“æœçŸ©é˜µ
        result = torch.eye(A.shape[-1], device=device, dtype=dtype).expand_as(A)
        term = torch.eye(A.shape[-1], device=device, dtype=dtype).expand_as(A)

        # æ³°å‹’çº§æ•°å±•å¼€
        for i in range(1, max_terms + 1):
            term = torch.matmul(term, A) / i
            result = result + term

        return result

    def build_adjacency_matrices_batch(self, edge_index, num_nodes, x):
        """æ‰¹é‡æ„å»ºæ‰€æœ‰èŠ‚ç‚¹çš„é‚»æ¥çŸ©é˜µ"""
        device = x.device
        dtype = torch.complex64

        # åˆ›å»ºå®Œæ•´çš„é‚»æ¥çŸ©é˜µ
        adj_matrix = torch.zeros(num_nodes, num_nodes, device=device, dtype=dtype)

        # å¡«å……é‚»æ¥çŸ©é˜µ
        adj_matrix[edge_index[0], edge_index[1]] = 1.0 + 0.0j

        # ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ›å»ºåŒ…å«è‡ªèº«å’Œé‚»å±…çš„å­å›¾é‚»æ¥çŸ©é˜µ
        # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç®€åŒ–çš„æ–¹æ³•ï¼šç›´æ¥ä½¿ç”¨åº¦æ•°ä½œä¸ºå­å›¾å¤§å°
        degrees = torch.zeros(num_nodes, device=device, dtype=torch.long)
        degrees = degrees.scatter_add(0, edge_index[0], torch.ones_like(edge_index[0]))
        degrees = degrees.scatter_add(0, edge_index[1], torch.ones_like(edge_index[1]))

        # é™åˆ¶æœ€å¤§é‚»å±…æ•°ä»¥æ§åˆ¶è®¡ç®—å¤æ‚åº¦
        max_neighbors = min(10, degrees.max().item())

        # åˆ›å»ºæ‰¹é‡é‚»æ¥çŸ©é˜µ (num_nodes, max_size, max_size)
        max_size = max_neighbors + 1  # +1 for the node itself
        batch_adj = torch.zeros(num_nodes, max_size, max_size, device=device, dtype=dtype)

        # ä¸ºæ¯ä¸ªèŠ‚ç‚¹æ„å»ºå­å›¾
        for node_idx in range(num_nodes):
            # æ‰¾åˆ°é‚»å±…
            neighbors = []
            mask = (edge_index[0] == node_idx) | (edge_index[1] == node_idx)
            if mask.any():
                node_edges = edge_index[:, mask]
                for src, dst in node_edges.t():
                    if src == node_idx:
                        neighbors.append(dst.item())
                    else:
                        neighbors.append(src.item())

            # å»é‡å¹¶é™åˆ¶æ•°é‡
            neighbors = list(set(neighbors))[:max_neighbors - 1]
            subgraph_nodes = [node_idx] + neighbors

            # æ„å»ºå­å›¾é‚»æ¥çŸ©é˜µ
            for i, node_i in enumerate(subgraph_nodes):
                for j, node_j in enumerate(subgraph_nodes):
                    if adj_matrix[node_i, node_j] != 0:
                        batch_adj[node_idx, i, j] = 1.0 + 0.0j

        return batch_adj

    def batch_unitary_evolution(self, batch_adj, complex_time):
        """æ‰¹é‡è®¡ç®—é…‰æ¼”åŒ–çŸ©é˜µ"""
        device = batch_adj.device
        num_nodes, max_size, _ = batch_adj.shape

        # è®¡ç®— -i * A * t
        evolution_arg = -1j * batch_adj * complex_time

        # ä½¿ç”¨æ³°å‹’çº§æ•°è®¡ç®—çŸ©é˜µæŒ‡æ•°
        G_t = self.matrix_exp_taylor(evolution_arg, self.max_matrix_exp_terms)

        # è®¡ç®—é…‰æ¼”åŒ–çŸ©é˜µçš„ç®€åŒ–ç‰ˆæœ¬
        # è¿™é‡Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨G_tçš„å½’ä¸€åŒ–ç‰ˆæœ¬
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è¿›è¡Œæ›´å¤æ‚çš„é…‰åŒ–å¤„ç†
        norms = torch.norm(G_t, dim=(-2, -1), keepdim=True)
        G_t_normalized = G_t / (norms + 1e-8)

        return G_t_normalized

    def batch_evolve_features(self, x, batch_adj, complex_time):
        """æ‰¹é‡æ¼”åŒ–èŠ‚ç‚¹ç‰¹å¾"""
        device = x.device
        num_nodes, feature_dim = x.shape
        max_size = batch_adj.shape[1]

        # è®¡ç®—æ¼”åŒ–çŸ©é˜µ
        U = self.batch_unitary_evolution(batch_adj, complex_time)

        # å‡†å¤‡ç‰¹å¾çŸ©é˜µ
        # ä¸ºæ¯ä¸ªèŠ‚ç‚¹å‡†å¤‡å…¶å­å›¾ç‰¹å¾
        batch_features = torch.zeros(num_nodes, max_size, feature_dim,
                                     device=device, dtype=x.dtype)

        # ç®€åŒ–ç‰ˆæœ¬ï¼šæ¯ä¸ªèŠ‚ç‚¹çš„ç¬¬ä¸€ä¸ªä½ç½®æ˜¯è‡ªå·±ï¼Œå…¶ä»–ä½ç½®å¡«å……é‚»å±…ç‰¹å¾çš„å¹³å‡å€¼
        batch_features[:, 0, :] = x

        # å¯¹äºå…¶ä»–ä½ç½®ï¼Œæˆ‘ä»¬ä½¿ç”¨é‚»åŸŸç‰¹å¾çš„èšåˆ
        for node_idx in range(num_nodes):
            # æ‰¾åˆ°é‚»å±…å¹¶èšåˆç‰¹å¾
            neighbors = []
            edge_mask = (batch_adj[node_idx, 0, 1:] != 0)
            if edge_mask.any():
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œä½¿ç”¨å¹³å‡æ± åŒ–
                neighbor_features = x.mean(dim=0, keepdim=True)
                for i in range(1, min(max_size, edge_mask.sum().item() + 1)):
                    batch_features[node_idx, i, :] = neighbor_features

        # åº”ç”¨æ¼”åŒ–çŸ©é˜µ
        # U: (num_nodes, max_size, max_size)
        # batch_features: (num_nodes, max_size, feature_dim)
        evolved_features = torch.matmul(U, batch_features)

        # è¿”å›æ¯ä¸ªèŠ‚ç‚¹æ¼”åŒ–åçš„ç‰¹å¾ï¼ˆå–ç¬¬ä¸€ä¸ªä½ç½®ï¼‰
        return evolved_features[:, 0, :]

    def complex_activation(self, x):
        """åº”ç”¨å¤æ•°æ¿€æ´»å‡½æ•°"""
        if self.activation == 'complex_relu':
            return complex_relu(x)
        elif self.activation == 'none':
            return x
        else:
            return complex_relu(x)

    def prepare_complex_input(self, x):
        """å°†å®æ•°è¾“å…¥è½¬æ¢ä¸ºå¤æ•°å½¢å¼"""
        if x.dtype in [torch.complex64, torch.complex128]:
            return x
        else:
            return torch.complex(x, torch.zeros_like(x))

    def forward(self, x, edge_index):
        num_nodes = x.size(0)

        # ç¡®ä¿è¾“å…¥æ˜¯å¤æ•°å½¢å¼
        x_complex = self.prepare_complex_input(x)

        # å¤æ•°ç‰¹å¾å˜æ¢
        x_transformed = self.complex_lin(x_complex)

        # åº”ç”¨å¤æ•°æ¿€æ´»å‡½æ•°
        x_transformed = self.complex_activation(x_transformed)

        # åº”ç”¨å¤æ•°dropout
        if self.training and self.dropout > 0:
            x_transformed = complex_dropout(x_transformed, p=self.dropout, training=self.training)

        # æ‰¹é‡æ„å»ºé‚»æ¥çŸ©é˜µ
        batch_adj = self.build_adjacency_matrices_batch(edge_index, num_nodes, x_transformed)

        # æ‰¹é‡æ¼”åŒ–ç‰¹å¾
        evolved_features = self.batch_evolve_features(x_transformed, batch_adj, self.complex_time_param)

        # æ·»åŠ é¢å¤–çš„å¤æ•°åç½®
        if self.complex_extra_bias is not None:
            evolved_features = evolved_features + self.complex_extra_bias

        return evolved_features


class OptimizedComplexUnitaryGCN(nn.Module):
    """ä¼˜åŒ–çš„å¤šå±‚å¤æ•°é…‰æ¼”åŒ–GCNç½‘ç»œ"""

    def __init__(self, input_dim, hidden_dims, output_dim,
                 evolution_times=None, dropout=0.0,
                 max_matrix_exp_terms=8):
        super(OptimizedComplexUnitaryGCN, self).__init__()

        # æ„å»ºå±‚åˆ—è¡¨
        dims = [input_dim] + hidden_dims + [output_dim]
        self.layers = nn.ModuleList()

        if evolution_times is None:
            evolution_times = [1.0] * (len(dims) - 1)

        for i in range(len(dims) - 1):
            layer = OptimizedComplexUnitaryEvolutionGCNConv(
                in_channels=dims[i],
                out_channels=dims[i + 1],
                evolution_time=evolution_times[i],
                dropout=dropout if i < len(dims) - 2 else 0.0,
                max_matrix_exp_terms=max_matrix_exp_terms
            )
            self.layers.append(layer)

    def forward(self, data):
        x, edge_index, batch = data.x, data.edge_index, data.batch

        # å‰å‘ä¼ æ’­
        for layer in self.layers:
            x = layer(x, edge_index)

        # å–å¹…åº¦å¹¶è¿›è¡Œå›¾æ± åŒ–
        x = x.abs()
        x = global_mean_pool(x, batch)
        x = F.log_softmax(x, dim=1)

        return x


# å•å±‚æ¶ˆæ¯ä¼ é€’ç»„ä»¶
class MessagePassingComplexUnitaryLayer(MessagePassing):
    """åŸºäºæ¶ˆæ¯ä¼ é€’çš„å¤æ•°é…‰æ¼”åŒ–å±‚ - å•å±‚å®ç°"""

    def __init__(self, in_channels, out_channels, evolution_time=1.0,
                 dropout=0.0, activation='complex_relu'):
        super(MessagePassingComplexUnitaryLayer, self).__init__(aggr='add')

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.dropout = dropout
        self.activation = activation

        # å¤æ•°çº¿æ€§å˜æ¢
        self.complex_lin = ComplexLinear(in_channels, out_channels)

        # æ¼”åŒ–å‚æ•°
        self.time_param_real = nn.Parameter(torch.tensor(evolution_time, dtype=torch.float32))
        self.time_param_imag = nn.Parameter(torch.zeros(1, dtype=torch.float32))

        # æ¶ˆæ¯ä¼ é€’çš„æƒé‡
        self.msg_lin = ComplexLinear(out_channels, out_channels)

    @property
    def complex_time_param(self):
        return torch.complex(self.time_param_real, self.time_param_imag)

    def message(self, x_j, x_i):
        """è®¡ç®—æ¶ˆæ¯ - æ¨¡æ‹Ÿé‡å­æ¼”åŒ–çš„æ¶ˆæ¯ä¼ é€’"""
        # åº”ç”¨å¤æ•°æ—¶é—´æ¼”åŒ–
        time_factor = torch.exp(-1j * self.complex_time_param)
        evolved_msg = self.msg_lin(x_j) * time_factor
        return evolved_msg

    def update(self, aggr_out, x):
        """æ›´æ–°èŠ‚ç‚¹ç‰¹å¾"""
        # ç»“åˆåŸå§‹ç‰¹å¾å’Œèšåˆæ¶ˆæ¯
        evolution_factor = torch.exp(-1j * self.complex_time_param * 0.5)
        updated = x * evolution_factor + aggr_out * (1 - evolution_factor)
        return updated

    def forward(self, x, edge_index):
        # ç¡®ä¿è¾“å…¥æ˜¯å¤æ•°å½¢å¼
        if x.dtype not in [torch.complex64, torch.complex128]:
            x = torch.complex(x, torch.zeros_like(x))

        # çº¿æ€§å˜æ¢
        x = self.complex_lin(x)

        # åº”ç”¨æ¿€æ´»å‡½æ•°
        if self.activation == 'complex_relu':
            x = complex_relu(x)

        # åº”ç”¨dropout
        if self.training and self.dropout > 0:
            x = complex_dropout(x, p=self.dropout, training=self.training)

        # æ¶ˆæ¯ä¼ é€’
        x = self.propagate(edge_index, x=x)

        return x


# å®Œæ•´çš„å¤šå±‚ç½‘ç»œ - ä¿®å¤æ¥å£é—®é¢˜
class MessagePassingComplexUnitaryGCN(nn.Module):
    """åŸºäºæ¶ˆæ¯ä¼ é€’çš„å¤æ•°é…‰æ¼”åŒ–GCNç½‘ç»œ - ç»Ÿä¸€æ¥å£"""

    def __init__(self, input_dim, hidden_dims, output_dim,
                 evolution_times=None, dropout=0.0, activation='complex_relu'):
        super(MessagePassingComplexUnitaryGCN, self).__init__()

        # æ„å»ºå±‚åˆ—è¡¨
        dims = [input_dim] + hidden_dims + [output_dim]
        self.layers = nn.ModuleList()

        if evolution_times is None:
            evolution_times = [1.0] * (len(dims) - 1)

        for i in range(len(dims) - 1):
            layer = MessagePassingComplexUnitaryLayer(
                in_channels=dims[i],
                out_channels=dims[i + 1],
                evolution_time=evolution_times[i],
                dropout=dropout if i < len(dims) - 2 else 0.0,
                activation=activation
            )
            self.layers.append(layer)

    def forward(self, data):
        """ç»Ÿä¸€çš„forwardæ¥å£ï¼Œæ¥å—dataå¯¹è±¡"""
        x, edge_index, batch = data.x, data.edge_index, data.batch

        # å‰å‘ä¼ æ’­
        for layer in self.layers:
            x = layer(x, edge_index)

        # å–å¹…åº¦å¹¶è¿›è¡Œå›¾æ± åŒ–
        x = x.abs()
        x = global_mean_pool(x, batch)
        x = F.log_softmax(x, dim=1)

        return x


# ä½¿ç”¨ç¤ºä¾‹å’Œæ€§èƒ½æµ‹è¯•
def test_optimized_models():
    """æµ‹è¯•ä¼˜åŒ–åçš„æ¨¡å‹æ€§èƒ½"""
    import time

    print("ğŸš€ GPUä¼˜åŒ–çš„å¤æ•°é…‰æ¼”åŒ–GCNæµ‹è¯•")
    print("=" * 50)

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    num_nodes = 1000
    num_features = 64
    num_classes = 10

    # ç”Ÿæˆéšæœºå›¾æ•°æ®
    x = torch.randn(num_nodes, num_features, device=device)

    # åˆ›å»ºéšæœºè¾¹
    num_edges = num_nodes * 5
    edge_index = torch.randint(0, num_nodes, (2, num_edges), device=device)

    # åˆ›å»ºbatchï¼ˆç”¨äºå›¾çº§åˆ«ä»»åŠ¡ï¼‰
    batch = torch.zeros(num_nodes, dtype=torch.long, device=device)

    class TestData:
        def __init__(self, x, edge_index, batch):
            self.x = x
            self.edge_index = edge_index
            self.batch = batch

    data = TestData(x, edge_index, batch)

    # æµ‹è¯•åŸå§‹æ¨¡å‹
    print("ğŸ“Š åŸå§‹æ¨¡å‹æ€§èƒ½æµ‹è¯•ï¼ˆCPUå¯†é›†å‹ï¼‰...")
    # è¿™é‡Œæˆ‘ä»¬ä¸è¿è¡ŒåŸå§‹æ¨¡å‹ï¼Œå› ä¸ºå®ƒä¼šå¾ˆæ…¢

    # æµ‹è¯•ä¼˜åŒ–æ¨¡å‹1
    print("ğŸ“Š ä¼˜åŒ–æ¨¡å‹1æ€§èƒ½æµ‹è¯•ï¼ˆæ‰¹é‡çŸ©é˜µè¿ç®—ï¼‰...")
    model1 = OptimizedComplexUnitaryGCN(
        input_dim=num_features,
        hidden_dims=[32, 16],
        output_dim=num_classes,
        dropout=0.1,
        max_matrix_exp_terms=6
    ).to(device)

    # é¢„çƒ­
    with torch.no_grad():
        _ = model1(data)

    # è®¡æ—¶
    start_time = time.time()
    for _ in range(10):
        with torch.no_grad():
            output1 = model1(data)
    end_time = time.time()

    print(f"   å¹³å‡æ¨ç†æ—¶é—´: {(end_time - start_time) / 10:.4f}s")
    print(f"   è¾“å‡ºå½¢çŠ¶: {output1.shape}")
    print(f"   GPUå†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated(device) / 1024 ** 2:.2f}MB")

    # æµ‹è¯•ä¼˜åŒ–æ¨¡å‹2
    print("ğŸ“Š ä¼˜åŒ–æ¨¡å‹2æ€§èƒ½æµ‹è¯•ï¼ˆæ¶ˆæ¯ä¼ é€’ï¼‰...")
    model2 = MessagePassingComplexUnitaryGCN(
        input_dim=num_features,
        hidden_dims=[32, 16],
        output_dim=num_classes,
        dropout=0.1
    ).to(device)

    # é¢„çƒ­
    with torch.no_grad():
        _ = model2(data)

    # è®¡æ—¶
    start_time = time.time()
    for _ in range(10):
        with torch.no_grad():
            output2 = model2(data)
    end_time = time.time()

    print(f"   å¹³å‡æ¨ç†æ—¶é—´: {(end_time - start_time) / 10:.4f}s")
    print(f"   è¾“å‡ºå½¢çŠ¶: {output2.shape}")
    print(f"   GPUå†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated(device) / 1024 ** 2:.2f}MB")

    # æµ‹è¯•æ¢¯åº¦è®¡ç®—
    print("ğŸ“Š æ¢¯åº¦è®¡ç®—æµ‹è¯•...")
    model2.train()
    optimizer = torch.optim.Adam(model2.parameters(), lr=0.01)

    output = model2(data)
    y_true = torch.randint(0, num_classes, (1,), device=device)  # å›¾çº§åˆ«æ ‡ç­¾
    loss = F.cross_entropy(output, y_true)

    start_time = time.time()
    loss.backward()
    optimizer.step()
    end_time = time.time()

    print(f"   åå‘ä¼ æ’­æ—¶é—´: {end_time - start_time:.4f}s")
    print(f"   æŸå¤±å€¼: {loss.item():.4f}")

    print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    test_optimized_models()