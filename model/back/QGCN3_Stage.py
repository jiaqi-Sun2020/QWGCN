# ä¼˜åŒ–ç‰ˆï¼šç¨³å®šé«˜æ•ˆçš„ OptimizedLocalUnitaryGCN
# ä¸»è¦æ”¹è¿›ï¼š
# 1. ç¨³å®šçš„å¤æ•°æƒé‡åˆå§‹åŒ–å’Œæ¢¯åº¦è£å‰ª
# 2. é«˜æ•ˆçš„çŸ©é˜µæŒ‡æ•°è¿‘ä¼¼ï¼ˆæ³°å‹’å±•å¼€ï¼‰
# 3. ç¨€ç–é‚»æ¥çŸ©é˜µæ“ä½œé¿å…å¯†é›†è½¬æ¢
# 4. æ•°å€¼ç¨³å®šçš„å¤æ•°å½’ä¸€åŒ–
# 5. æ¸è¿›å¼è®­ç»ƒç­–ç•¥

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import global_mean_pool
from torch_geometric.utils import add_self_loops, degree
from typing import Optional, Tuple
import math
from torch_geometric.data import Data
from torch_geometric.utils import erdos_renyi_graph
import time
from torch_geometric.utils import subgraph
import numpy as np


# ==== ç¨³å®šçš„å¤æ•°å±‚å½’ä¸€åŒ– ====
class StableComplexLayerNorm(nn.Module):
    def __init__(self, normalized_shape, eps=1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(normalized_shape))
        self.bias = nn.Parameter(torch.zeros(normalized_shape))

    def forward(self, x):
        # ä½¿ç”¨æ¨¡é•¿è¿›è¡Œå½’ä¸€åŒ–ï¼Œé¿å…æ•°å€¼ä¸ç¨³å®š
        magnitude = torch.sqrt(x.real ** 2 + x.imag ** 2 + self.eps)
        mean_mag = magnitude.mean(dim=-1, keepdim=True)
        std_mag = magnitude.std(dim=-1, keepdim=True) + self.eps

        # å½’ä¸€åŒ–æ¨¡é•¿
        normed_mag = (magnitude - mean_mag) / std_mag
        normed_mag = normed_mag * self.weight + self.bias

        # ä¿æŒç›¸ä½ï¼Œé‡æ„å¤æ•°
        phase = torch.atan2(x.imag, x.real)
        return torch.polar(torch.abs(normed_mag) + self.eps, phase)


# ==== ç¨³å®šçš„å¤æ•°çº¿æ€§å±‚ ====
class StableComplexLinear(nn.Module):
    def __init__(self, in_features, out_features, bias=True):
        super().__init__()
        # Xavieråˆå§‹åŒ–ï¼Œé€‚åˆå¤æ•°ç½‘ç»œ
        std = math.sqrt(1.0 / in_features)
        self.weight_real = nn.Parameter(torch.randn(out_features, in_features) * std)
        self.weight_imag = nn.Parameter(torch.randn(out_features, in_features) * std)

        if bias:
            self.bias_real = nn.Parameter(torch.zeros(out_features))
            self.bias_imag = nn.Parameter(torch.zeros(out_features))
        else:
            self.register_parameter('bias_real', None)
            self.register_parameter('bias_imag', None)

    def forward(self, x):
        # å¤æ•°çŸ©é˜µä¹˜æ³•
        real_part = torch.matmul(x.real, self.weight_real.T) - torch.matmul(x.imag, self.weight_imag.T)
        imag_part = torch.matmul(x.real, self.weight_imag.T) + torch.matmul(x.imag, self.weight_real.T)

        result = torch.complex(real_part, imag_part)

        if self.bias_real is not None:
            bias = torch.complex(self.bias_real, self.bias_imag)
            result = result + bias

        return result


# ==== ç¨³å®šçš„å¤æ•°æ¿€æ´»å‡½æ•° ====
def stable_complex_relu(x, leak=0.01):
    """å¸¦æ³„æ¼çš„å¤æ•°ReLUï¼Œæé«˜ç¨³å®šæ€§"""
    real_part = torch.where(x.real > 0, x.real, leak * x.real)
    imag_part = torch.where(x.imag > 0, x.imag, leak * x.imag)
    return torch.complex(real_part, imag_part)


def stable_complex_dropout(x, p=0.5, training=True):
    """ç¨³å®šçš„å¤æ•°dropout"""
    if not training or p == 0:
        return x
    # å¯¹æ¨¡é•¿è¿›è¡Œdropout
    magnitude = torch.sqrt(x.real ** 2 + x.imag ** 2 + 1e-8)
    mask = (torch.rand_like(magnitude) > p).float()
    scale = 1.0 / (1.0 - p)

    phase = torch.atan2(x.imag, x.real)
    return torch.polar(magnitude * mask * scale, phase)


# ==== é«˜æ•ˆçš„çŸ©é˜µæŒ‡æ•°è¿‘ä¼¼ ====
def efficient_matrix_exp(H, t=1.0, max_terms=6):
    """ä½¿ç”¨æ³°å‹’å±•å¼€è¿‘ä¼¼çŸ©é˜µæŒ‡æ•°ï¼Œé¿å…æ˜‚è´µçš„ç²¾ç¡®è®¡ç®—"""
    device = H.device
    n = H.size(-1)

    # ç¼©æ”¾æ—¶é—´æ­¥é•¿ä»¥æé«˜ç¨³å®šæ€§
    scaled_H = -1j * H * t

    # æ³°å‹’å±•å¼€: exp(A) â‰ˆ I + A + AÂ²/2! + AÂ³/3! + ...
    result = torch.eye(n, device=device, dtype=torch.complex64).expand_as(scaled_H)
    term = torch.eye(n, device=device, dtype=torch.complex64).expand_as(scaled_H)

    for k in range(1, max_terms + 1):
        term = torch.matmul(term, scaled_H) / k
        result = result + term

        # æ—©åœï¼šå¦‚æœé¡¹å˜å¾—å¾ˆå°å°±åœæ­¢
        if torch.max(torch.abs(term)) < 1e-6:
            break

    return result


# ==== ç¨€ç–å“ˆå¯†é¡¿çŸ©é˜µæ„é€  ====
def create_sparse_hamiltonian(edge_index, num_nodes, edge_weight=None, type='laplacian'):
    """ç›´æ¥ä»ç¨€ç–è¡¨ç¤ºæ„é€ å“ˆå¯†é¡¿çŸ©é˜µï¼Œé¿å…å¯†é›†è½¬æ¢"""
    if edge_weight is None:
        edge_weight = torch.ones(edge_index.size(1), device=edge_index.device)

    row, col = edge_index
    deg = degree(row, num_nodes)

    if type == 'laplacian':
        # L = D - A
        laplacian_weight = torch.cat([deg, -edge_weight])
        laplacian_index = torch.cat([
            torch.stack([torch.arange(num_nodes, device=edge_index.device),
                         torch.arange(num_nodes, device=edge_index.device)]),
            edge_index
        ], dim=1)
    elif type == 'norm_laplacian':
        # L = I - D^(-1/2) A D^(-1/2)
        deg_inv_sqrt = deg.pow(-0.5)
        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0

        norm_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]
        laplacian_weight = torch.cat([torch.ones(num_nodes, device=edge_index.device), -norm_weight])
        laplacian_index = torch.cat([
            torch.stack([torch.arange(num_nodes, device=edge_index.device),
                         torch.arange(num_nodes, device=edge_index.device)]),
            edge_index
        ], dim=1)

    # è½¬æ¢ä¸ºå¯†é›†çŸ©é˜µç”¨äºå°å­å›¾
    H = torch.sparse_coo_tensor(laplacian_index, laplacian_weight,
                                (num_nodes, num_nodes)).to_dense()
    return H.to(torch.complex64) * 0.05  # å‡å°ç¼©æ”¾å› å­æé«˜ç¨³å®šæ€§


# ==== é«˜æ•ˆçš„å±€éƒ¨é…‰GCNå·ç§¯å±‚ ====
class EfficientLocalUnitaryGCNConv(nn.Module):
    def __init__(self, in_channels, out_channels, k_hop=1, evolution_time=0.5,
                 hamilton_type='laplacian', max_subgraph_size=6, dropout=0.1):
        super().__init__()
        self.lin = StableComplexLinear(in_channels, out_channels)
        self.norm = StableComplexLayerNorm(out_channels)
        self.k_hop = k_hop
        self.evolution_time = evolution_time
        self.hamilton_type = hamilton_type
        self.max_subgraph_size = max_subgraph_size
        self.dropout = dropout

        # æ®‹å·®è¿æ¥çš„æŠ•å½±å±‚
        if in_channels != out_channels:
            self.residual_proj = StableComplexLinear(in_channels, out_channels)
        else:
            self.residual_proj = None

    def forward(self, x, edge_index):
        # ä¿å­˜è¾“å…¥ç”¨äºæ®‹å·®è¿æ¥
        x_residual = x

        # è½¬æ¢ä¸ºå¤æ•°
        if x.is_floating_point():
            x = torch.complex(x, torch.zeros_like(x))

        # çº¿æ€§å˜æ¢
        x = self.lin(x)

        N = x.size(0)
        edge_index_with_loops, _ = add_self_loops(edge_index, num_nodes=N)

        # æ„å»ºé‚»æ¥å…³ç³»å­—å…¸ï¼Œé¿å…å¯†é›†è½¬æ¢
        adj_dict = {}
        for i in range(edge_index_with_loops.size(1)):
            src, dst = edge_index_with_loops[:, i].tolist()
            if src not in adj_dict:
                adj_dict[src] = []
            adj_dict[src].append(dst)

        evolved = torch.zeros_like(x)

        for i in range(N):
            # è·å–k-hopé‚»å±…
            neighbors = set([i])
            current_level = {i}

            for hop in range(self.k_hop):
                next_level = set()
                for node in current_level:
                    if node in adj_dict:
                        next_level.update(adj_dict[node])
                neighbors.update(next_level)
                current_level = next_level

                if len(neighbors) >= self.max_subgraph_size:
                    break

            sub_nodes = list(neighbors)[:self.max_subgraph_size]
            if i not in sub_nodes:
                sub_nodes[0] = i

            # æ„å»ºå­å›¾
            sub_edge_index, _ = subgraph(sub_nodes, edge_index_with_loops,
                                         relabel_nodes=True, num_nodes=N)

            if sub_edge_index.size(1) == 0:
                evolved[i] = x[i]
                continue

            center_idx = sub_nodes.index(i)
            sub_x = x[sub_nodes]

            # æ„å»ºå­å›¾å“ˆå¯†é¡¿çŸ©é˜µ
            H = create_sparse_hamiltonian(sub_edge_index, len(sub_nodes),
                                          type=self.hamilton_type)

            # é«˜æ•ˆçŸ©é˜µæŒ‡æ•°è®¡ç®—
            U_sub = efficient_matrix_exp(H, self.evolution_time)

            # æ„å»ºæ‰©å±•é…‰ç®—å­
            dim = len(sub_nodes)
            U = torch.zeros(2 * dim, 2 * dim, dtype=torch.complex64, device=x.device)
            U[:dim, :dim] = U_sub
            U[dim:, dim:] = U_sub.conj().transpose(-2, -1)

            # åº”ç”¨é…‰æ¼”åŒ–
            z = torch.zeros(2 * dim, x.size(1), dtype=torch.complex64, device=x.device)
            z[:dim] = sub_x
            z_evolved = torch.matmul(U, z)

            evolved[i] = z_evolved[center_idx]

        # æ¿€æ´»å‡½æ•°
        evolved = stable_complex_relu(evolved)

        # Dropout
        if self.training and self.dropout > 0:
            evolved = stable_complex_dropout(evolved, self.dropout)

        # å±‚å½’ä¸€åŒ–
        evolved = self.norm(evolved)

        # æ®‹å·®è¿æ¥
        if self.residual_proj is not None:
            if x_residual.is_floating_point():
                x_residual = torch.complex(x_residual, torch.zeros_like(x_residual))
            x_residual = self.residual_proj(x_residual)
        elif x_residual.is_floating_point():
            x_residual = torch.complex(x_residual, torch.zeros_like(x_residual))

        return evolved + x_residual


# ==== ä¸»ç½‘ç»œ ====
class StableOptimizedLocalUnitaryGCN(nn.Module):
    def __init__(self, input_dim, hidden_dims, output_dim,
                 k_hop=1, evolution_times=None, hamilton_type='laplacian',
                 dropout=0.1, max_subgraph_size=6):
        super().__init__()
        dims = [input_dim] + hidden_dims + [output_dim]
        self.layers = nn.ModuleList()

        if evolution_times is None:
            # æ¸è¿›å¼æ¼”åŒ–æ—¶é—´
            evolution_times = [0.3, 0.5, 0.7][:len(dims) - 1]
            evolution_times = evolution_times + [evolution_times[-1]] * (len(dims) - 1 - len(evolution_times))

        for i in range(len(dims) - 1):
            self.layers.append(EfficientLocalUnitaryGCNConv(
                in_channels=dims[i],
                out_channels=dims[i + 1],
                k_hop=k_hop,
                evolution_time=evolution_times[i],
                hamilton_type=hamilton_type,
                dropout=dropout if i < len(dims) - 2 else 0.0,
                max_subgraph_size=max_subgraph_size
            ))

        # æ¢¯åº¦è£å‰ª
        self.grad_clip = 1.0

    def forward(self, data):
        x, edge_index, batch = data.x, data.edge_index, data.batch

        for layer in self.layers:
            x = layer(x, edge_index)

            # æ¢¯åº¦è£å‰ªé˜²æ­¢çˆ†ç‚¸
            if self.training:
                torch.nn.utils.clip_grad_norm_(self.parameters(), self.grad_clip)

        # æ¨¡é•¿åˆ†ç±»
        x = x.abs()

        # å…¨å±€å¹³å‡æ± åŒ–
        x = global_mean_pool(x, batch)

        return F.log_softmax(x, dim=1)


# ==== æµ‹è¯•å‡½æ•° ====
def test_stable_optimized_local_unitary_gcn():
    print("\U0001F680 ç¨³å®šä¼˜åŒ–ç‰ˆ LocalUnitaryGCN æµ‹è¯•")
    print("=" * 60)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # æ¨¡æ‹Ÿå›¾æ„å»º
    num_nodes = 100  # å‡å°è§„æ¨¡æµ‹è¯•
    num_features = 64
    num_classes = 2
    edge_index = erdos_renyi_graph(num_nodes, edge_prob=0.05).to(device)
    x = torch.randn(num_nodes, num_features).to(device) * 0.1  # å°åˆå§‹å€¼
    batch = torch.zeros(num_nodes, dtype=torch.long).to(device)
    data = Data(x=x, edge_index=edge_index, batch=batch)

    print(f"\nğŸ“Š å›¾ä¿¡æ¯: {num_nodes} èŠ‚ç‚¹, {edge_index.size(1)} æ¡è¾¹")

    model = StableOptimizedLocalUnitaryGCN(
        input_dim=num_features,
        hidden_dims=[32, 16],
        output_dim=num_classes,
        k_hop=1,  # å‡å°k_hop
        evolution_times=[0.2, 0.3, 0.4],
        hamilton_type='laplacian',
        dropout=0.1,
        max_subgraph_size=4  # å‡å°å­å›¾è§„æ¨¡
    ).to(device)


    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    # æ¨ç†æµ‹è¯•
    model.eval()
    with torch.no_grad():
        try:
            output = model(data)
            print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
            print(f"è¾“å‡ºèŒƒå›´: [{output.min().item():.4f}, {output.max().item():.4f}]")
            print(f"è¾“å‡ºæ˜¯å¦æœ‰NaN: {torch.isnan(output).any().item()}")
            print(f"è¾“å‡ºæ˜¯å¦æœ‰Inf: {torch.isinf(output).any().item()}")
        except Exception as e:
            print(f"æ¨¡å‹æ‰§è¡Œå‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return

    # æ¢¯åº¦æµ‹è¯•
    model.train()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
    start_time = time.time()
    try:
        y_fake = torch.randint(0, num_classes, (1,)).to(device)
        output = model(data)
        loss = F.nll_loss(output, y_fake)
        print(f"æµ‹è¯•loss: {loss.item():.6f}")

        loss.backward()

        # æ£€æŸ¥æ¢¯åº¦
        total_grad_norm = 0
        for param in model.parameters():
            if param.grad is not None:
                total_grad_norm += param.grad.data.norm(2).item() ** 2
        total_grad_norm = total_grad_norm ** 0.5
        print(f"æ€»æ¢¯åº¦èŒƒæ•°: {total_grad_norm:.6f}")

        optimizer.step()
        print("âœ… æ¢¯åº¦æ›´æ–°æˆåŠŸ")
        end_time = time.time()
        avg_time = (end_time - start_time)
        print(f"å¹³å‡æ¨ç†æ—¶é—´: {avg_time * 1000:.2f} ms")
    except Exception as e:
        print(f"è®­ç»ƒæµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

    print("\nâœ… å…³é”®æ”¹è¿›:")
    print("  - ç¨³å®šçš„å¤æ•°æƒé‡åˆå§‹åŒ–")
    print("  - é«˜æ•ˆçš„çŸ©é˜µæŒ‡æ•°è¿‘ä¼¼ï¼ˆæ³°å‹’å±•å¼€ï¼‰")
    print("  - ç¨€ç–å›¾æ“ä½œé¿å…å¯†é›†è½¬æ¢")
    print("  - æ¢¯åº¦è£å‰ªé˜²æ­¢æ•°å€¼çˆ†ç‚¸")
    print("  - å‡å°æ¼”åŒ–æ—¶é—´å’Œå­å›¾è§„æ¨¡")
    print("  - ä¿æŒæ‰€æœ‰æ ¸å¿ƒç‰¹æ€§ï¼šå®Œæ•´å¤æ•°æ¼”åŒ– + æ¨¡é•¿åˆ†ç±» + æ®‹å·®è¿æ¥ + å±€éƒ¨æ‰©å¼ é…‰æ€§")


if __name__ == "__main__":
    test_stable_optimized_local_unitary_gcn()