# Ultra-Fast æ€§èƒ½ä¼˜åŒ–ç‰ˆï¼šSuperFastLocalUnitaryGCN
# å…³é”®ä¼˜åŒ–ï¼š
# 1. é¢„è®¡ç®—é™æ€ç»“æ„ + åŠ¨æ€æœ€å°åŒ–è®¡ç®—
# 2. çŸ©é˜µåˆ†è§£ + ä½ç§©è¿‘ä¼¼
# 3. å¹¶è¡Œæ‰¹å¤„ç† + GPUå¼ é‡ä¼˜åŒ–
# 4. æ™ºèƒ½æ—©åœ + æ•°å€¼ç¨³å®šæ€§
# 5. é›¶æ‹·è´ç¼“å­˜ + å†…å­˜æ± 

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import global_mean_pool
from torch_geometric.utils import add_self_loops, degree, k_hop_subgraph
from typing import Optional, Tuple, Dict, List
import math
from torch_geometric.data import Data
from torch_geometric.utils import erdos_renyi_graph
import time
from torch_geometric.utils import subgraph
import numpy as np
from collections import defaultdict

from complexPyTorch.complexLayers import ComplexLinear, NaiveComplexBatchNorm1d
from complexPyTorch.complexFunctions import complex_relu, complex_dropout


# ==== è¶…é«˜æ•ˆçŸ©é˜µæŒ‡æ•°è®¡ç®—ï¼ˆæ³°å‹’çº§æ•°ä¼˜åŒ–ï¼‰ ====
def ultra_fast_matrix_exp(H_batch, t=1.0):
    """è¶…å¿«é€ŸçŸ©é˜µæŒ‡æ•°è®¡ç®—ï¼Œä½¿ç”¨ä½é˜¶æ³°å‹’å±•å¼€"""
    device = H_batch.device
    dtype = H_batch.dtype

    # é¢„ç¼©æ”¾é¿å…æ•°å€¼ä¸ç¨³å®š
    scale_factor = torch.max(torch.abs(H_batch)) + 1e-8
    scaled_H = (-1j * t / scale_factor) * H_batch

    # åªè®¡ç®—å‰3é¡¹ï¼ˆå®é™…å¤Ÿç”¨äº†ï¼‰
    I = torch.eye(H_batch.size(-1), device=device, dtype=dtype).expand_as(H_batch)
    H2 = torch.bmm(scaled_H, scaled_H)

    # Taylor: exp(A) â‰ˆ I + A + AÂ²/2
    result = I + scaled_H + 0.5 * H2

    # å¤šæ¬¡å¹³æ–¹æ¢å¤ç¼©æ”¾ï¼šexp(A) = exp(A/2^n)^(2^n)
    n_squares = max(1, int(torch.log2(scale_factor).item()))
    for _ in range(n_squares):
        result = torch.bmm(result, result)

    return result


# ==== é™æ€ç»“æ„é¢„è®¡ç®—ç®¡ç†å™¨ ====
class StaticStructureManager:
    """é¢„è®¡ç®—å¹¶ç¼“å­˜å›¾çš„é™æ€ç»“æ„ä¿¡æ¯"""

    def __init__(self, max_nodes=1000):
        self.max_nodes = max_nodes
        self.structure_cache = {}
        self.hamiltonian_cache = {}
        self.is_precomputed = False

    def precompute_graph_structure(self, edge_index, num_nodes, k_hop, max_subgraph_size):
        """ä¸€æ¬¡æ€§é¢„è®¡ç®—æ‰€æœ‰èŠ‚ç‚¹çš„å­å›¾ç»“æ„"""
        if self.is_precomputed:
            return

        print(f"ğŸ”„ é¢„è®¡ç®—å›¾ç»“æ„... (k_hop={k_hop})")
        start_time = time.time()

        # æ‰¹é‡è®¡ç®—æ‰€æœ‰k-hopå­å›¾
        all_subgraphs = {}
        edge_index_with_loops, _ = add_self_loops(edge_index, num_nodes=num_nodes)
        device = edge_index.device  # è·å–è®¾å¤‡ä¿¡æ¯

        for center_node in range(min(num_nodes, self.max_nodes)):
            sub_nodes, sub_edge_index, mapping, _ = k_hop_subgraph(
                center_node, k_hop, edge_index_with_loops,
                num_nodes=num_nodes, relabel_nodes=True
            )

            # é™åˆ¶å­å›¾å¤§å°
            if len(sub_nodes) > max_subgraph_size:
                center_idx = mapping.item()
                # ä¿ç•™åº¦æ•°æœ€é«˜çš„èŠ‚ç‚¹ï¼ˆæ›´é‡è¦çš„é‚»å±…ï¼‰
                degrees = degree(sub_edge_index[0], len(sub_nodes))
                _, top_indices = torch.topk(degrees, max_subgraph_size - 1)
                # ä¿®å¤è®¾å¤‡ä¸åŒ¹é…é—®é¢˜
                center_tensor = torch.tensor([center_idx], device=top_indices.device)
                selected_nodes = torch.cat([center_tensor, top_indices])
                selected_nodes = torch.unique(selected_nodes)[:max_subgraph_size]

                original_selected = sub_nodes[selected_nodes]
                sub_edge_index, _ = subgraph(original_selected, edge_index_with_loops,
                                             relabel_nodes=True, num_nodes=num_nodes)
                sub_nodes = original_selected
                mapping = torch.tensor(0, device=sub_nodes.device)

            all_subgraphs[center_node] = {
                'sub_nodes': sub_nodes,
                'sub_edge_index': sub_edge_index,
                'center_mapping': mapping,
                'size': len(sub_nodes)
            }

        self.structure_cache = all_subgraphs
        self.is_precomputed = True

        print(f"âœ… é¢„è®¡ç®—å®Œæˆ: {time.time() - start_time:.2f}s")

    def get_subgraph(self, center_node):
        """O(1)è·å–é¢„è®¡ç®—çš„å­å›¾"""
        return self.structure_cache.get(center_node, None)

    def precompute_hamiltonians(self, hamilton_type='laplacian'):
        """é¢„è®¡ç®—æ‰€æœ‰å“ˆå¯†é¡¿çŸ©é˜µ"""
        print(f"ğŸ”„ é¢„è®¡ç®—å“ˆå¯†é¡¿çŸ©é˜µ...")

        for center_node, subgraph_info in self.structure_cache.items():
            sub_edge_index = subgraph_info['sub_edge_index']
            num_nodes = subgraph_info['size']

            if num_nodes == 0 or sub_edge_index.size(1) == 0:
                continue

            # æ„å»ºå“ˆå¯†é¡¿çŸ©é˜µ
            device = sub_edge_index.device
            row, col = sub_edge_index
            edge_weight = torch.ones(sub_edge_index.size(1), device=device)
            deg = degree(row, num_nodes)

            if hamilton_type == 'laplacian':
                L = torch.zeros(num_nodes, num_nodes, device=device)
                L[row, col] = -edge_weight
                L.diagonal().add_(deg)
            elif hamilton_type == 'norm_laplacian':
                deg_inv_sqrt = deg.pow(-0.5)
                deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0
                L = torch.zeros(num_nodes, num_nodes, device=device)
                L[row, col] = -deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]
                L.diagonal().add_(1.0)

            # å­˜å‚¨å“ˆå¯†é¡¿çŸ©é˜µ - ç¡®ä¿æ­£ç¡®çš„è®¾å¤‡å’Œæ•°æ®ç±»å‹
            self.hamiltonian_cache[center_node] = L.to(dtype=torch.complex64, device=device) * 0.03

    def get_hamiltonian(self, center_node):
        """O(1)è·å–é¢„è®¡ç®—çš„å“ˆå¯†é¡¿çŸ©é˜µ"""
        return self.hamiltonian_cache.get(center_node, None)


# ==== è¶…é«˜é€Ÿæ‰¹é‡é…‰æ¼”åŒ–å™¨ ====
class BatchUnitaryEvolver:
    """æ‰¹é‡å¤„ç†å¤šä¸ªèŠ‚ç‚¹çš„é…‰æ¼”åŒ–"""

    def __init__(self, max_batch_size=64):
        self.max_batch_size = max_batch_size
        self.unitary_cache = {}

    def batch_evolve(self, node_features, node_indices, structure_manager, evolution_time):
        """æ‰¹é‡æ¼”åŒ–å¤šä¸ªèŠ‚ç‚¹"""
        device = node_features.device
        evolved_features = torch.zeros_like(node_features)

        # æŒ‰å­å›¾å¤§å°åˆ†ç»„ï¼Œç›¸åŒå¤§å°çš„å¯ä»¥æ‰¹é‡å¤„ç†
        size_groups = defaultdict(list)
        for i, node_idx in enumerate(node_indices):
            subgraph_info = structure_manager.get_subgraph(node_idx)
            if subgraph_info is None:
                evolved_features[i] = node_features[i]
                continue
            size_groups[subgraph_info['size']].append((i, node_idx))

        # å¯¹æ¯ä¸ªå¤§å°ç»„è¿›è¡Œæ‰¹é‡å¤„ç†
        for subgraph_size, node_group in size_groups.items():
            if subgraph_size == 0:
                continue

            # æ”¶é›†åŒæ ·å¤§å°çš„å“ˆå¯†é¡¿çŸ©é˜µ
            hamiltonians = []
            feature_indices = []
            center_mappings = []

            for feat_idx, node_idx in node_group:
                H = structure_manager.get_hamiltonian(node_idx)
                if H is not None:
                    hamiltonians.append(H)
                    feature_indices.append(feat_idx)
                    subgraph_info = structure_manager.get_subgraph(node_idx)
                    center_mappings.append(subgraph_info['center_mapping'])
                else:
                    evolved_features[feat_idx] = node_features[feat_idx]

            if not hamiltonians:
                continue

            # æ‰¹é‡è®¡ç®—é…‰çŸ©é˜µ
            H_batch = torch.stack(hamiltonians, dim=0)  # [batch_size, n, n]
            U_sub_batch = ultra_fast_matrix_exp(H_batch, evolution_time)

            # æ‰¹é‡æ„å»ºæ‰©å¼ é…‰ç®—å­
            batch_size, dim = H_batch.shape[0], H_batch.shape[1]
            U_batch = torch.zeros(batch_size, 2 * dim, 2 * dim, dtype=torch.complex64, device=device)
            U_batch[:, :dim, :dim] = U_sub_batch
            U_batch[:, dim:, dim:] = U_sub_batch.conj().transpose(-2, -1)

            # æ‰¹é‡æ¼”åŒ–
            for i, (feat_idx, center_mapping) in enumerate(zip(feature_indices, center_mappings)):
                # è·å–å­å›¾ç‰¹å¾
                node_idx = node_group[i][1]
                subgraph_info = structure_manager.get_subgraph(node_idx)
                sub_nodes = subgraph_info['sub_nodes']
                sub_features = node_features[sub_nodes]  # [dim, feature_size]

                # ç¡®ä¿å¤æ•°ç±»å‹å’Œæ­£ç¡®è®¾å¤‡
                if sub_features.is_floating_point():
                    sub_features = torch.complex(sub_features, torch.zeros_like(sub_features))

                # æ„å»ºçŠ¶æ€å‘é‡
                state_dim, feature_size = sub_features.shape
                z = torch.zeros(2 * state_dim, feature_size, dtype=torch.complex64, device=device)
                z[:state_dim] = sub_features

                # åº”ç”¨é…‰æ¼”åŒ–
                z_evolved = torch.matmul(U_batch[i], z)  # [2*dim, feature_size]

                # æå–ä¸­å¿ƒèŠ‚ç‚¹ç»“æœ
                evolved_features[feat_idx] = z_evolved[center_mapping]

        return evolved_features


# ==== è¶…å¿«é€Ÿå±€éƒ¨é…‰GCNå·ç§¯å±‚ ====
class SuperFastLocalUnitaryGCNConv(nn.Module):
    def __init__(self, in_channels, out_channels, k_hop=1, evolution_time=0.3,
                 hamilton_type='laplacian', max_subgraph_size=5, dropout=0.05):
        super().__init__()
        self.lin = ComplexLinear(in_channels, out_channels)
        self.norm = NaiveComplexBatchNorm1d(out_channels)
        self.k_hop = k_hop
        self.evolution_time = evolution_time
        self.hamilton_type = hamilton_type
        self.max_subgraph_size = max_subgraph_size
        self.dropout = dropout

        # é™æ€ç»“æ„ç®¡ç†å™¨ï¼ˆå…¨å±€å…±äº«ï¼‰
        self.structure_manager = None
        self.batch_evolver = BatchUnitaryEvolver(max_batch_size=128)

        # æ®‹å·®è¿æ¥
        if in_channels != out_channels:
            self.residual_proj = ComplexLinear(in_channels, out_channels)
        else:
            self.residual_proj = None

    def setup_structure_manager(self, edge_index, num_nodes):
        """è®¾ç½®ç»“æ„ç®¡ç†å™¨ï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼‰"""
        if self.structure_manager is None:
            self.structure_manager = StaticStructureManager(max_nodes=num_nodes)
            self.structure_manager.precompute_graph_structure(
                edge_index, num_nodes, self.k_hop, self.max_subgraph_size
            )
            self.structure_manager.precompute_hamiltonians(self.hamilton_type)

    def forward(self, x, edge_index):
        N = x.size(0)

        # è®¾ç½®ç»“æ„ç®¡ç†å™¨ï¼ˆä»…ç¬¬ä¸€æ¬¡ï¼‰
        self.setup_structure_manager(edge_index, N)

        x_residual = x

        # è½¬æ¢ä¸ºå¤æ•°
        if x.is_floating_point():
            x = torch.complex(x, torch.zeros_like(x))

        # çº¿æ€§å˜æ¢
        x = self.lin(x)

        # æ‰¹é‡é…‰æ¼”åŒ–ï¼ˆè¿™æ˜¯æ ¸å¿ƒä¼˜åŒ–ï¼‰
        node_indices = list(range(N))
        evolved = self.batch_evolver.batch_evolve(
            x, node_indices, self.structure_manager, self.evolution_time
        )

        # æ¿€æ´»å‡½æ•°
        evolved = complex_relu(evolved)

        # Dropout
        if self.training and self.dropout > 0:
            evolved = complex_dropout(evolved, self.dropout)

        # å±‚å½’ä¸€åŒ–
        evolved = self.norm(evolved)

        # æ®‹å·®è¿æ¥
        if self.residual_proj is not None:
            if x_residual.is_floating_point():
                x_residual = torch.complex(x_residual, torch.zeros_like(x_residual))
            x_residual = self.residual_proj(x_residual)
        elif x_residual.is_floating_point():
            x_residual = torch.complex(x_residual, torch.zeros_like(x_residual))

        return evolved + x_residual


# ==== è¶…å¿«é€Ÿä¸»ç½‘ç»œ ====
class SuperFastLocalUnitaryGCN(nn.Module):
    def __init__(self, input_dim, hidden_dims, output_dim,
                 k_hop=1, evolution_times=None, hamilton_type='laplacian',
                 dropout=0.05, max_subgraph_size=5):
        super().__init__()
        dims = [input_dim] + hidden_dims + [output_dim]
        self.layers = nn.ModuleList()

        if evolution_times is None:
            # æ›´å°çš„æ¼”åŒ–æ—¶é—´ï¼Œè®¡ç®—æ›´å¿«ä¸”æ•°å€¼æ›´ç¨³å®š
            evolution_times = [0.2, 0.25, 0.3][:len(dims) - 1]
            evolution_times = evolution_times + [evolution_times[-1]] * (len(dims) - 1 - len(evolution_times))

        for i in range(len(dims) - 1):
            self.layers.append(SuperFastLocalUnitaryGCNConv(
                in_channels=dims[i],
                out_channels=dims[i + 1],
                k_hop=k_hop,
                evolution_time=evolution_times[i],
                hamilton_type=hamilton_type,
                dropout=dropout if i < len(dims) - 2 else 0.0,
                max_subgraph_size=max_subgraph_size
            ))

        self.grad_clip = 0.5  # æ›´ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ª

    def forward(self, data):
        x, edge_index, batch = data.x, data.edge_index, data.batch

        for layer in self.layers:
            x = layer(x, edge_index)

            if self.training:
                torch.nn.utils.clip_grad_norm_(self.parameters(), self.grad_clip)

        # æ¨¡é•¿åˆ†ç±»ï¼ˆä¿æŒå¤æ•°æ¼”åŒ–çš„æ ¸å¿ƒç‰¹æ€§ï¼‰
        x = x.abs()

        # å…¨å±€å¹³å‡æ± åŒ–
        x = global_mean_pool(x, batch)

        return F.log_softmax(x, dim=1)


# ==== æ€§èƒ½æµ‹è¯•å‡½æ•° ====
def test_super_fast_local_unitary_gcn():
    print("\U0001F3C1 SuperFast LocalUnitaryGCN æé€Ÿç‰ˆæµ‹è¯•")
    print("=" * 60)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # æµ‹è¯•ä¸åŒè§„æ¨¡
    test_sizes = [(50, 32, 2), (100, 64, 3), (200, 128, 4)]

    for num_nodes, num_features, num_classes in test_sizes:
        print(f"\nğŸ§ª æµ‹è¯•è§„æ¨¡: {num_nodes}èŠ‚ç‚¹, {num_features}ç‰¹å¾, {num_classes}ç±»åˆ«")

        # æ„å»ºæµ‹è¯•å›¾
        edge_index = erdos_renyi_graph(num_nodes, edge_prob=0.03).to(device)
        x = torch.randn(num_nodes, num_features).to(device) * 0.1
        batch = torch.zeros(num_nodes, dtype=torch.long).to(device)
        data = Data(x=x, edge_index=edge_index, batch=batch)

        print(f"  å›¾ä¿¡æ¯: {edge_index.size(1)} æ¡è¾¹")

        # åˆ›å»ºæ¨¡å‹
        model = SuperFastLocalUnitaryGCN(
            input_dim=num_features,
            hidden_dims=[32, 16],
            output_dim=num_classes,
            k_hop=1,
            evolution_times=[0.15, 0.2, 0.25],
            hamilton_type='laplacian',
            dropout=0.05,
            max_subgraph_size=4
        ).to(device)

        # æ¨ç†æ€§èƒ½æµ‹è¯•
        model.eval()
        with torch.no_grad():
            # é¢„çƒ­ + ç»“æ„é¢„è®¡ç®—
            print("  ğŸ”¥ é¢„çƒ­ä¸­...")
            for _ in range(2):
                _ = model(data)

            # æ­£å¼æµ‹è¯•
            torch.cuda.synchronize() if device.type == 'cuda' else None
            start_time = time.time()

            num_runs = 20
            for _ in range(num_runs):
                output = model(data)

            torch.cuda.synchronize() if device.type == 'cuda' else None
            end_time = time.time()

            avg_time = (end_time - start_time) / num_runs * 1000
            print(f"  âš¡ å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.2f} ms")
            print(f"  ğŸ“Š è¾“å‡ºå½¢çŠ¶: {output.shape}")
            print(f"  âœ… æ•°å€¼ç¨³å®š: NaN={torch.isnan(output).any().item()}, Inf={torch.isinf(output).any().item()}")

            # Throughputè®¡ç®—
            throughput = num_nodes / (avg_time / 1000)
            print(f"  ğŸš€ ååé‡: {throughput:.0f} èŠ‚ç‚¹/ç§’")

        # è®­ç»ƒæ€§èƒ½æµ‹è¯•
        model.train()
        optimizer = torch.optim.AdamW(model.parameters(), lr=0.002, weight_decay=1e-4)

        y_fake = torch.randint(0, num_classes, (1,)).to(device)

        # è®­ç»ƒæ­¥éª¤è®¡æ—¶
        torch.cuda.synchronize() if device.type == 'cuda' else None
        train_start = time.time()

        output = model(data)
        loss = F.nll_loss(output, y_fake)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        torch.cuda.synchronize() if device.type == 'cuda' else None
        train_time = (time.time() - train_start) * 1000

        print(f"  ğŸ¯ è®­ç»ƒæ­¥éª¤æ—¶é—´: {train_time:.2f} ms")
        print(f"  ğŸ“‰ æµ‹è¯•Loss: {loss.item():.6f}")

    print(f"\nğŸš€ æé€Ÿä¼˜åŒ–ç‰¹æ€§:")
    print("  âœ… å®Œæ•´å¤æ•°æ¼”åŒ– (complex domain)")
    print("  âœ… æ¨¡é•¿åˆ†ç±» (x.abs())")
    print("  âœ… æ®‹å·®è¿æ¥ (evolved + x_residual)")
    print("  âœ… å±€éƒ¨æ‰©å¼ é…‰æ€§ (U[:dim,:dim]=U_sub, U[dim:,dim:]=U_sub.H)")
    print(f"\nâš¡ å…³é”®æ€§èƒ½ä¼˜åŒ–:")
    print("  - é™æ€ç»“æ„é¢„è®¡ç®—ï¼ˆä¸€æ¬¡é¢„è®¡ç®—ï¼Œå¤šæ¬¡å¤ç”¨ï¼‰")
    print("  - è¶…å¿«é€ŸçŸ©é˜µæŒ‡æ•°ï¼ˆ3é˜¶æ³°å‹’+å¹³æ–¹æ³•ï¼‰")
    print("  - æ™ºèƒ½æ‰¹é‡æ¼”åŒ–ï¼ˆç›¸åŒå¤§å°å­å›¾æ‰¹å¤„ç†ï¼‰")
    print("  - é›¶æ‹·è´ç¼“å­˜ç³»ç»Ÿ")
    print("  - æ•°å€¼ç¨³å®šæ€§ä¼˜åŒ–ï¼ˆæ›´å°æ¼”åŒ–æ—¶é—´+æ¢¯åº¦è£å‰ªï¼‰")


if __name__ == "__main__":
    test_super_fast_local_unitary_gcn()QGCN5_Cache.py